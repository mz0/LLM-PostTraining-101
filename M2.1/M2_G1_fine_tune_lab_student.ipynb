{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85ea228",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Graded Lab: Fine-Tuning Lab\n",
    "\n",
    "Welcome to the assignment for this module!\n",
    "\n",
    "Carefully read each Markdown (text) cell, which include instructions and hints. Start by reading the background behind your upcoming tasks.\n",
    "\n",
    "When you are done, submit your solution by saving it, then clicking on the submit button at the top right side of the page.\n",
    "\n",
    "## In order for your submission to be graded correctly, you **MUST**:\n",
    "* **Use the provided variable names**, otherwise the autograder will not be able to locate the variable for grading. \n",
    "\n",
    "* **Replace any instances of `None` with your own code.** \n",
    "\n",
    "* **Only modify the cells that start with the comment `# GRADED CELL`**.  \n",
    "\n",
    "* **Use the provided cells for your solution.** You can add new cells to experiment, but these will be omitted when grading. \n",
    "\n",
    "To submit your solution, save it, then click on the blue submit button at the top of the page.\n",
    "\n",
    "<div style=\"background-color: #FAD888; padding: 10px; border-radius: 3px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); width:95%\n",
    "\">\n",
    "<strong>Important notes</strong>:\n",
    "\n",
    "- Code blocks with None will not run properly. If you run them before completing the exercise, you will likely get an error. \n",
    "\n",
    "- The notebooks work best in Chrome browser. If you are having problems, please switch to Chrome.\n",
    "\n",
    "- Make sure you always save before submitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a22a4a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will gain practical experience improving a model through fine-tuning.\n",
    "\n",
    "You will start with a model that has already been pre-trained on a large corpus of text. Your goal is to improve its performance on math related questions. Your investigation will involve learning how to prepare inputs for a language model. You will then use the HuggingFace library to tune the model, and observe the effect that tuning hyperparameters has on the final model.\n",
    "\n",
    "## Objectives\n",
    "* **Embeddings**: Get information from a model's token embeddings.\n",
    "* **Padding**: Create padded batches for prompts of differing lengths.\n",
    "* **Explore Appropriate Learning Rates**: Plot loss curves to find an appropriate learning rate for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ef756",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Setup](#setup)\n",
    "* [Tokenization](#tokenization) - Exercise 1\n",
    "* [Padding](#padding) - Exercise 2\n",
    "* [HuggingFace Trainer](#hftrainer)\n",
    "* [Fine-tuning](#supervisedfine-tuning) - Exercise 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af823f9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setup <a id=\"setup\"></a>\n",
    "\n",
    "Start by importing all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928643e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# these two packages are from HuggingFace [https://huggingface.co/]\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from utils import extract_numerical_answer, plot_train_vs_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713294f08eba146b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next you will load the dataset. You will be using the same one from the previous assignment, **GSM8K**. It is already split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee165bd838c5b85c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/app/data/gsm8k', 'main')\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29857419ef194d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's take a look at one of the questions from the dataset using the function below. The function will print the question, its associated answer, and the extracted answer. The `extracted_numerical_anser` function is just like the one you built in the previous lab.\n",
    "\n",
    "Try looking at a few questions. You can create a new cell and use the function and change the index (`idx`) to see different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad45b579499741",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def display_data_point(idx):\n",
    "    example = train[idx]\n",
    "    print(f'Plaintext Question:\\n```\\n{example['question']}\\n```\\n')\n",
    "    print(f'Plaintext Answer:\\n```\\n{example['answer']}\\n```\\n')\n",
    "    print(f'Extracted Answer:\\n```\\n{extract_numerical_answer(example['answer'])}\\n```\\n')\n",
    "\n",
    "display_data_point(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a833c003588ef8a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, you'll load a language `model` using the `get_model` function. This function initializes both a model and its corresponding tokenizer.\n",
    "\n",
    "**Key Concepts**\n",
    "- **Tokenizer**: Processes strings into a format the model can understand\n",
    "- **Model-specific**: Each tokenizer is unique to its model - they must match!\n",
    "- **Model choice**: We're using DeepSeek Math base, an open-source 7-billion parameter model specialized for mathematical reasoning\n",
    "\n",
    "The function returns:\n",
    "- `model`: The loaded language model ready for inference\n",
    "- `tokenizer`: The preprocessing tool for converting text to model inputs\n",
    "\n",
    "> **Note**: This function typically takes around 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdfe1134be3a6d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    # Get the tokenizer that corresponds to the model you will use\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "\n",
    "    # Get the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.bfloat16, device_map='auto')\n",
    "\n",
    "    # Specify beginning of sequence token, end of sequence token, and padding token\n",
    "    model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_name = '/app/models/deepseek-ai/deepseek-math-7b-base'\n",
    "model, tokenizer = get_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c2096",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Tokenization <a id=\"tokenization\"></a>\n",
    "\n",
    "Strings must be converted into a form suitable for the language model before being passed. The `tokenizer` accomplishes this by taking the string and chunking it up into \"tokens\". For the DeepSeek model, a token is typically a whole word along with a preceding space. However, longer or less common words may be chunked into multiple tokens.\n",
    "\n",
    "Each token has an id, and these ids are passed into the model. Note that each model might use a different set of tokens with different ids, so each model needs to use an appropriate tokenizer. That's why you get the tokenizer together with the model, to make sure they fit together.\n",
    "\n",
    "Run the following cell to see how a standard prompt is tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6498655",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def token_table(prompt, tokenizer):\n",
    "    # Tokenize the prompt\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "\n",
    "    # Decode each token individually to get its representation\n",
    "    token_strings = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "\n",
    "    # Create the token table\n",
    "    column_width = 21\n",
    "    # Create and print the header\n",
    "    header = f\"{'Token Ids':^{column_width}}\" + f\"{'Token Strings':^{column_width}}\"\n",
    "    print(header)\n",
    "    print(\"=\" * len(header))\n",
    "    # Print the tokens row by row\n",
    "    for token_id, token_string in zip(token_ids, token_strings):\n",
    "        # Format token string with quotes\n",
    "        token_string_quoted = f\"'{token_string}'\"\n",
    "        print(f\"{token_id:^{column_width}}\" + f\"{token_string_quoted:^{column_width}}\")\n",
    "\n",
    "prompt = 'The integral of x^2 from 0 to 2 is'\n",
    "token_table(prompt, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858b767",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You saw how you can use the `tokenizer` to obtain the token ids and the token strings. But how precisely do you pass this into the `model`? You can just invoke the `tokenizer` directly to obtain an object which contains the ids that the `model` will accept and understand. As all neural networks, this model works with numbers and does not need the token strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218dece",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da466d1dd854a779",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Notice how the tokenizer also outputs an `attention_mask`. This is necessary to tell the `model` which tokens to pay attention to (`1`) and which to ignore (`0`). Notice how they are all `1` currently, but you will see an example later when you use padding, where some `0`s appear.\n",
    "\n",
    "You typically include the following argument `return_tensors='pt'` to specify that you want the data to be represented as tensors instead of Python lists. A tensor is similar to a list, but it stores its data in a way that allow faster computations. It also is able to be put on the AMD GPU for even more speed. The string `'pt'` stands for \"PyTorch tensor\" because PyTorch is the library used to define the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ecdb0e053fa7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "tokenizer(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc75bad48cfbe6d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You can now pass this into the `model`! The `model.generate(**ids, max_new_tokens=5)` function is used to generate new tokens using a greedy method (picking the highest probability token next). Then the tokenizer decodes the tokens to create a more human-friendly output.\n",
    "\n",
    "Run the cell below. Does the model answer the question correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eb5eac347f97bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def model_output(prompt, model, tokenizer):\n",
    "\n",
    "    # Using `inference mode` avoids overhead usually required during training\n",
    "    # The code still works without `inference mode`, but requires more memory\n",
    "    with torch.inference_mode():\n",
    "        ids = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=ids['input_ids'],\n",
    "            attention_mask=ids['attention_mask'],\n",
    "            max_new_tokens=5)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f'Model output: {outputs}')\n",
    "        print(f'Model decoded output: {decoded}\\n')\n",
    "\n",
    "    token_table(decoded, tokenizer)\n",
    "\n",
    "model_output(prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2338d69788710",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 1: Investigating embedding shape\n",
    "\n",
    "When you pass a token id to the `model`, it is used to look up a list of numbers (a.k.a. a \"vector\" or \"embedding\"). These embeddings are what the model really uses internally as inputs. This is because embeddings are more expressive as inputs than mere ids are, and the model will have an easier time learning.\n",
    "\n",
    "Below, write code to investigate the number of such embeddings (called \"vocab size\") that the model has internally as well as the length of each embedding (called \"embedding dimension\").\n",
    "\n",
    "The `embeddings` are technically a \"layer\" of the model. Layers typically have an attribute `.weight` which is the PyTorch tensor you are interested in. PyTorch tensors themselves have a `.shape` attribute which outputs the length of each axis.\n",
    "\n",
    "For example, if `a` is a PyTorch tensor,\n",
    "```python\n",
    "tensor([[1.2,  3.5, -0.8],\n",
    "        [2.3, -3.1, -1.8]])\n",
    "```\n",
    "then `a.shape` will return the tuple `(2, 3)` because there are 2\n",
    "rows and 3 columns.\n",
    "\n",
    "Assign the `emb_shape` variable to a tuple representing the shape of the `embedding` layer's weight tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0188d0f952d5f",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 1\n",
    "\n",
    "def get_embedding_info(embeddings):\n",
    "    \"\"\"\n",
    "    Get basic information about the model's token embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    emb_shape = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    vocab_size = emb_shape[0]\n",
    "    embedding_dim = emb_shape[1]\n",
    "    \n",
    "    return vocab_size, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f35231",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the next cell to perform a simple test of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52dc196",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the function\n",
    "vocab_size, embedding_dim = get_embedding_info(model.get_input_embeddings())\n",
    "\n",
    "# Verification checks\n",
    "assert vocab_size > 50000, \"Vocabulary size of modern models should be over 50,000\"\n",
    "assert embedding_dim > 512, \"Embedding dimension of modern models should be over 512\"\n",
    "print(f'Success! The model has {vocab_size:,} tokens, each represented by {embedding_dim}-dimensional vectors')\n",
    "print(\n",
    "    'Trivia: This \"embedding layer\" essentially encodes what list of numbers should represent each token, '\n",
    "    f'and it was learned during pre-training. It contains {vocab_size * embedding_dim:,} learnable parameters!'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573dbc4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Padding <a id=\"padding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f512d264ce35586",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 2: Padded batches\n",
    "\n",
    "When training, you can process multiple inputs at once for efficiency. Text inputs naturally have different lengths, but this is problematic because tensors require all sequences in a batch to have the same length. This is where padding comes in. You can apply a special \"padding\" token to shorter sequences to make all sequences the same length.\n",
    "\n",
    "For example, assume you have these sequences of tokens.\n",
    "```python\n",
    "              ['The', 'cat', 'in',  'the', 'hat']\n",
    "                            ['AI',  'is',  'fun']\n",
    "['Fei', 'Fei', 'Li',  'ran', 'an',  'AI',  'lab']\n",
    "```\n",
    "Padding to the longest sequence would produce the following\n",
    "```python\n",
    "['PAD', 'PAD', 'The', 'cat', 'in',  'the', 'hat']\n",
    "['PAD', 'PAD', 'PAD', 'PAD', 'AI',  'is',  'fun']\n",
    "['Fei', 'Fei', 'Li',  'ran', 'an',  'AI',  'lab']\n",
    "```\n",
    "\n",
    "Complete the function below to create a properly padded batch from text prompts of different lengths.\n",
    "\n",
    "Your are given `prompts` which is a list of prompts of different lengths. You are also given a `tokenizer`.\n",
    "\n",
    "You previously passed a single prompt to the tokenizer. Now, you can pass the whole list of prompts at once (no loop required). You should invoke the `tokenizer` directly with `tokenizer(prompts, ...)`. There is NO need to call another function like `tokenizer.encode(...)` nor `tokenizer.decode(...)`. Include the argument `return_tensors='pt'` as before, but now also include `padding=True`.\n",
    "\n",
    "The output of the `tokenizer` will be a dictionary-like object which represents a batch. Get the value for the `'input_ids'` key with `batch['input_ids']`, and assign it to the corresponding variable for the return statement. Do the same for the `'attention_mask'` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5faacace2e598",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 2\n",
    "\n",
    "def create_padded_batch(prompts, tokenizer):\n",
    "    \"\"\"\n",
    "    Create a padded batch from prompts of different lengths by passing the prompts to the tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = torch.tensor(0)\n",
    "    attention_mask = torch.tensor(0)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    batch = None\n",
    "    input_ids = None\n",
    "    attention_mask = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8726a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the function\n",
    "test_prompts = [\n",
    "    \"What is 2+2?\",\n",
    "    \"Calculate the derivative of x^3 + 2x^2 - 5x + 7 with respect to x\",\n",
    "    \"Find the integral of sin(x)cos(x)dx\"\n",
    "]\n",
    "\n",
    "input_ids, attention_mask = create_padded_batch(test_prompts, tokenizer)\n",
    "\n",
    "# Check the results\n",
    "assert len(input_ids.shape) == 2, 'Tensors should be 2-dimensional (with first axis being the batch dimension)'\n",
    "assert input_ids.shape == attention_mask.shape, 'input_ids and attention_mask should have the same shape'\n",
    "assert input_ids.dtype == attention_mask.dtype == torch.int64\n",
    "assert input_ids.shape[0] == len(test_prompts), 'Batch size should match number of test prompts'\n",
    "assert input_ids.shape[1] == max(len(tokenizer.encode(s, add_special_tokens=True)) for s in test_prompts), f'Padding should be to the longest sequence'\n",
    "assert torch.all((0 <= input_ids) & (input_ids <= model.get_input_embeddings().weight.shape[0])), f'Some ids in `input_ids` are invalid'\n",
    "assert torch.all((attention_mask == 0) | (attention_mask == 1)), 'Attention mask should only contain 0s and 1s'\n",
    "print(f'Success! Created batch with shape: {input_ids.shape}')\n",
    "print(f'Padding token ID used: {tokenizer.pad_token_id}\\n')\n",
    "print(f'{input_ids = }')\n",
    "print(f'{attention_mask = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a980512be7776b7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## HuggingFace Trainer <a id=\"hftrainer\"></a>\n",
    "\n",
    "The code here sets up a fine-tuning pipeline that teaches the `model` to answer questions. Many of the details of tokenization and padding are handled internally by the framework. Also, the trainer object will be responsible for looping over the data, computing a loss from the model, backpropagating, and repeating.\n",
    "\n",
    "The `get_trainer()` function returns the `SFTTrainer()`, which you have learned about in the videos and takes care of the supervised Fine-Tuning. This abstraction is made to simplify your code later on. Note that the `completion_only_loss` is set to `True` within the `SFTConfig()`, which makes sure that the loss is only calculated on the outputs, but not on the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8543c62068110",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def get_trainer(model_name, learning_rate, weight_decay=0.01, num_train_epochs=3, train_set_size=200, eval_set_size=20):\n",
    "    batch_size = 20\n",
    "    train_dataset = train.select(range(train_set_size)).rename_columns({\n",
    "        'question': 'prompt', \n",
    "        'answer': 'completion'\n",
    "    })\n",
    "    eval_dataset = test.select(range(eval_set_size)).rename_columns({\n",
    "        'question': 'prompt', \n",
    "        'answer': 'completion'\n",
    "    })\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        # Logging\n",
    "        output_dir=None,\n",
    "        logging_dir=None,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=1,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=1,\n",
    "        save_strategy='no',\n",
    "        report_to='none',\n",
    "        # Hyperparameters\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        # Dataset\n",
    "        completion_only_loss=True,\n",
    "    )\n",
    "\n",
    "    return SFTTrainer(\n",
    "        model=model_name,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b6e66bfeebfd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Fine-Tuning <a id=\"supervisedfine-tuning\"></a>\n",
    "\n",
    "Get the trainer with chosen hyperparameters, call the trainer which loops over your dataset to tune your model, and then plot the loss curves. Loss is a measure of how badly the model does, so the lower the better.\n",
    "\n",
    "> **Note**: The cell typically takes around 60 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991557fc55b2ede",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(model_name, learning_rate=9e-7)\n",
    "trainer.train()\n",
    "plot_train_vs_val(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ce376",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The `model` is not doing very well! Notice that the loss hardly descends at all for the validation loss. The training loss seems a bit more erratic but it is also hardly descending at all. This slow descent is a sign that the learning rate is too low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84224318",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 3: Finding a good learning rate\n",
    "\n",
    "Your goal is to tune the learning rate so that the loss curve descends faster. The curve below is the kind of curve you are going for!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edccabf2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![Good Plot](good-plot-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453661ef7e4774e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Your task is to assign the `learning_rate_ex3` variable to an appropriate value. Afterwards, the cell creates a trainer, performs training, and plots. Try different learning rates to see what results you get, and choose the learning rate that produces a good loss curve.\n",
    "\n",
    "> **Note**: The cell typically takes around 60 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08eef8d68701cb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 3\n",
    "\n",
    "# Hint: Your learning rate needs to be higher than what you have seen above.\n",
    "### START CODE HERE ###\n",
    "learning_rate_ex3 = None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f5be119fe8fe1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(model_name, learning_rate=learning_rate_ex3)\n",
    "trainer.train()\n",
    "plot_train_vs_val(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7329fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert 3e-6 <= trainer.args.learning_rate <= 1e-5\n",
    "print()\n",
    "print(f'Success! You have found an appropriate range for the learning rate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43eee665346f8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 4: Finding a learning rate that's too high!\n",
    "\n",
    "You've seen an example of the learning rate being too low and another example of it being tuned correctly. Now try to tune the learning rate too high by assigning `learning_rate_ex4` and see what effect this has on the loss.\n",
    "\n",
    "> **Note**: The cell typically takes around 60 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d37e68edcad6a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 4\n",
    "\n",
    "### START CODE HERE ###\n",
    "learning_rate_ex4 = None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260cd3509eb88b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(model_name, learning_rate=learning_rate_ex4)\n",
    "trainer.train()\n",
    "plot_train_vs_val(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f32c2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert 9e-5 <= trainer.args.learning_rate\n",
    "print()\n",
    "print(f'Success! You have found a learning rate that was too large.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb08eed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Congratulations! You've succesfully completed the SFT lab!"
   ]
  }
 ],
 "metadata": {
  "grader_version": "2",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
