1. The primary role of evaluation tests is to actively guide and steer post-training development by identifying model strengths and weaknesses.

Correct
Evaluation tests serve as an active feedback loop, highlighting areas of strength and weakness to inform ongoing improvements after training.

2. What is the main purpose of RL test environments when evaluating reinforcement learning for language models?

RL test environments ensure reliable, reproducible evaluation of RL-trained language models and help detect reward hacking.

Correct
RL test environments are designed to provide deterministic, reproducible conditions and to reveal issues like reward hacking that may not appear in standard test sets.

3. Which sequence best describes the steps involved in effective error analysis for language models?

Review failures, cluster errors, develop targeted fixes, run experiments, and repeat error analysis on new results.

Correct
This process ensures errors are systematically identified, addressed with targeted fixes, and iteratively re-evaluated to continuously improve the model.

4. Which of the following best describes when an evaluation is representative of real user interactions?

An evaluation is representative when it matches the real topics, distributions, and behaviors of actual user interactions.

Correct
A representative evaluation reflects the actual types of questions, usage patterns, and frequency distributions seen in real user scenarios.

5. Why is it recommended to begin with small, targeted evaluation sets before expanding coverage when evaluating model performance?

Starting with small, targeted evaluation sets enables rapid, actionable insights before scaling to broader, more reliable coverage.

Correct
Small, focused evaluation sets allow you to quickly identify issues and make improvements, building a solid foundation before expanding to more comprehensive assessments.

6. Why does reward hacking occur when designing reward functions for reinforcement learning?

Reward hacking occurs when the reward function is misaligned with true goals, so the model games the reward instead.

Correct
Reward hacking happens because the reward function does not perfectly capture the intended objectives, leading the model to exploit loopholes for high rewards.

7. Why is loss considered insufficient as the main metric for evaluating language models after training?

Loss only measures next-token prediction, not real user experience or desired behaviors after language model training.

Correct
Loss focuses on next-token accuracy, but doesn't reflect whether the model meets user needs or behaves as intended in practical scenarios.

8. Red teaming tests language models by intentionally trying to break their safety and robustness through adversarial and harmful prompts.

Correct
Red teaming involves actively probing models with adversarial or harmful inputs to uncover vulnerabilities in safety and robustness.

9. How do adversarial and real-world testing scenarios help reveal weaknesses in language models?

Adversarial and real-world testing expose model vulnerabilities by simulating attacks and unexpected user behaviors to reveal weaknesses.

Correct
Adversarial and real-world testing are designed to surface vulnerabilities by challenging the model with harmful prompts and novel user tactics.

10. How do calibration and uncertainty metrics contribute to evaluating the outputs of language models?

Calibration and uncertainty metrics assess if model confidence matches reality and help identify when outputs may be unreliable.

Correct
These metrics help determine whether the model's confidence scores are trustworthy and can highlight cases where outputs should not be relied upon.
