{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5211a77e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Graded Lab: Evaluation and Debugging Lab\n",
    "\n",
    "Welcome to this assignment!\n",
    "\n",
    "Carefully read each Markdown (text) cell, which include instructions and hints. Start by reading the background behind your upcoming tasks.\n",
    "\n",
    "When you are done, submit your solution by saving it, then clicking on the submit button at the top right side of the page.\n",
    "\n",
    "## In order for your submission to be graded correctly, you **MUST**:\n",
    "* **Use the provided variable names**, otherwise the autograder will not be able to locate the variable for grading. \n",
    "\n",
    "* **Replace any instances of `None` with your own code.** \n",
    "\n",
    "* **Only modify the cells that start with the comment `# GRADED CELL`**.  \n",
    "\n",
    "* **Use the provided cells for your solution.** You can add new cells to experiment, but these will be omitted when grading. \n",
    "\n",
    "To submit your solution, save it, then click on the blue submit button at the top of the page.\n",
    "\n",
    "<div style=\"background-color: #FAD888; padding: 10px; border-radius: 3px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); width:95%\n",
    "\">\n",
    "<strong>Important note</strong>: Code blocks with None will not run properly. If you run them before completing the exercise, you will likely get an error. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9fadf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will build a systematic pipeline to evaluate, analyze, and improve Large Language Models on mathematical reasoning tasks. This assignment will step you through the complete process of identifying model weaknesses and using targeted fine-tuning to address them.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You'll diagnose model failures, generate relevant training data, and measure improvement effectiveness using real mathematical reasoning tasks.\n",
    "\n",
    "* **Evaluate Model Performance:** Implement a comprehensive evaluation pipeline to test model accuracy on mathematical problems and establish baseline performance metrics.\n",
    "* **Analyze Error Categories:** Build an error classification system that categorizes model mistakes into specific types (calculation errors, reasoning errors, format errors) using language model analysis.\n",
    "* **Generate Targeted Training Data:** Use embedding-based similarity search to identify and select relevant training examples that match the specific error patterns found in the evaluation phase.\n",
    "* **Fine-tune Model with Targeted Data:** Implement supervised fine-tuning using the targeted training examples to address identified weaknesses and improve model performance on mathematical reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfaaf9b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Table of Contents\n",
    "- [Setup](#setup)\n",
    "- [Evaluate the Model](#section_evaluate) - Exercise 1\n",
    "- [Analyze the Errors](#section_analyze) - Exercise 2\n",
    "- [Error Clustering](#errorclustering)\n",
    "- [Training Data Generation](#section_generate) - Exercise 3\n",
    "- [Model Fine-tuning](#section_tune) - Exercise 4\n",
    "- [Model Re-evaluation](#modelreevaluation)\n",
    "- [Model Accuracy Comparison](#modelaccuracycomparison)\n",
    "- [Final Results Summary](#finalresultssummary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697225d4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e1f41",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Setup <a id=\"setup\"></a>\n",
    "\n",
    "As usual, start by importing packages and setting up the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d407b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Disable progress bars to avoid Jupyter context errors\n",
    "os.environ['HF_DATASETS_DISABLE_PROGRESS_BAR'] = '1'\n",
    "import warnings\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, GenerationConfig, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Also disable programmatically\n",
    "import datasets\n",
    "datasets.disable_progress_bar()\n",
    "\n",
    "\n",
    "import sys\n",
    "_parent_dir = os.path.abspath('..')\n",
    "if _parent_dir not in sys.path:\n",
    "    sys.path.append(_parent_dir)\n",
    "# Import timing functions from utils (relative to notebook)\n",
    "from utils.utils import extract_numerical_answer, extract_final_answer, get_model, generate_batch_responses, load_error_analysis_model, cluster_errors, cached_analyze_errors, load_error_analysis_cache, create_cache_key, load_evaluation_cache, cached_evaluate_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set up environment\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9a61a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af235bc8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load GSM8K dataset\n",
    "# Note: You may see XET progress bar errors - these are harmless warnings and can be ignored\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = io.StringIO()\n",
    "\n",
    "dataset = load_from_disk('/app/data/gsm8k', 'main')\n",
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "eval_dataset = test.select(range(300))\n",
    "\n",
    "example = test[0]\n",
    "print(f'Question:\\n{example[\"question\"]}')\n",
    "print()\n",
    "print(f'Answer:\\n{example[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954059c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5812fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# # Load the math model\n",
    "model_name = \"/app/models/deepseek-math-7b-base\"\n",
    "model, tokenizer = get_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4a662",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4d14b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Evaluate the Model <a id=\"section_evaluate\"></a>\n",
    "\n",
    "You will test how well your model performs on math problems before you try to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8d8b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 1: Evaluate Model Performance\n",
    "\n",
    "This section sets up the evaluation pipeline for testing model performance.\n",
    "\n",
    "The `evaluate_model` function:\n",
    "- Extracts questions and answers from the dataset\n",
    "- Generates model predictions in batches\n",
    "- Compares predictions to correct answers\n",
    "- Calculates accuracy\n",
    "- Stores results for analysis\n",
    "\n",
    "**CAREFUL! There are two parts where you need to enter your own code in this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aac418",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 1\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, model_name: str) -> Dict:\n",
    "    \"\"\"Evaluate model on dataset.\"\"\"\n",
    "    \n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    ### START CODE HERE ###\n",
    "    # Extract questions and correct answers from the dataset into a list\n",
    "    # The dataset is a list of dictionaries, each containing a 'question' key.\n",
    "    # Use list comprehension to iterate over each dictionary (example) in the dataset\n",
    "    # Access each question with example['question'] inside the list comprehension\n",
    "    questions = None\n",
    "    \n",
    "    # Similarly, extract the numerical answer by passing example['answer'] to extract_numerical_answer()inside the list comprehension\n",
    "    correct_answers = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Generate responses using batch processing\n",
    "    print(\"üöÄ Generating responses...\") \n",
    "    model_responses = generate_batch_responses(model, tokenizer, questions, batch_size=8)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_count = 0\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'predictions': [],\n",
    "        'correct_answers': correct_answers,\n",
    "        'questions': questions,\n",
    "        'model_responses': model_responses,\n",
    "        'correct': []\n",
    "    }\n",
    "    \n",
    "    for i, (correct_answer, response) in enumerate(zip(correct_answers, model_responses)):\n",
    "        \n",
    "        predicted_answer = extract_final_answer(response)\n",
    "        \n",
    "        ### START CODE HERE ###  \n",
    "        try:\n",
    "            # Convert the correct and predicted answer to float.\n",
    "            correct_num = None\n",
    "            predicted_num = None\n",
    "            # Check if the difference between correct_answer and predicted_answer < 1e-3\n",
    "            is_correct = None\n",
    "        except:\n",
    "            is_correct = False \n",
    "        ### END CODE HERE ###  \n",
    "\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "            \n",
    "        results['predictions'].append(predicted_answer)\n",
    "        results['correct'].append(is_correct)\n",
    "\n",
    "    # Calculate final accuracy\n",
    "    results['accuracy'] = correct_count / len(dataset)\n",
    "\n",
    "    print(f\"‚úÖ Accuracy: {results['accuracy']:.3f} ({correct_count}/{len(dataset)})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84cc16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Mathematical reasoning is challenging for LLMs. Expect baseline accuracies around 30-70% depending on problem complexity.\n",
    "\n",
    "This baseline establishes your starting point. All improvements will be measured against these results.\n",
    "\n",
    "Run the next cell to evaluate the original model and establish your baseline.\n",
    "\n",
    "It should take no more than 7 minutes to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eeef61",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create cache key from questions in the dataset\n",
    "questions = [example['question'] for example in eval_dataset]\n",
    "cache_key = create_cache_key(\"Original Model\", questions)\n",
    "\n",
    "# Try to load from cache first - if exists, use it directly without calling evaluation function\n",
    "results = load_evaluation_cache(cache_key)\n",
    "\n",
    "if results is None:\n",
    "    # Only call the expensive evaluation function if no cache exists\n",
    "    results = cached_evaluate_model(evaluate_model, model, tokenizer, eval_dataset, \"Original Model\")\n",
    "# Save results (same as before)\n",
    "os.makedirs('lab_results', exist_ok=True)\n",
    "with open('lab_results/evaluation_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'accuracy': results['accuracy'],\n",
    "        'total_examples': len(results['correct']),\n",
    "        'correct_predictions': sum(results['correct'])\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Show some examples of incorrect predictions\n",
    "incorrect_indices = [i for i, correct in enumerate(results['correct']) if not correct]\n",
    "print(f\"\\nFound {len(incorrect_indices)} incorrect predictions\")\n",
    "\n",
    "print(\"\\nExample incorrect predictions:\")\n",
    "for i, idx in enumerate(incorrect_indices[:3]):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {results['questions'][idx][:100]}...\")\n",
    "    print(f\"Correct: {results['correct_answers'][idx]}\")\n",
    "    print(f\"Predicted: {results['predictions'][idx]}\")\n",
    "\n",
    "print(f\"\\nüìä Original Model Accuracy: {results['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa115222",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Analyze the Errors <a id=\"section_analyze\"></a>\n",
    "\n",
    "You will look at what types of errors the model is making so you can fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab7f97",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Error analysis is crucial for targeted improvements. The first cell will load a helper model, and the second will define your error categorization function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd789143",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 2: Analyze Error Categories\n",
    "\n",
    "Complete the `analyze_error` function with your implementation.\n",
    "\n",
    "\n",
    "To generate a good prompt for error analysis using a language model, make sure your prompt does the following:\n",
    "\n",
    "- **Includes the key parts of the math problem**:\n",
    "  - The **QUESTION** \n",
    "  - The **CORRECT** answer.\n",
    "  - The **PREDICTED** answer (i.e. the model‚Äôs or student's answer).\n",
    "  \n",
    "- **Asks the model to categorize the error**, using **exactly one** of the options provided in `ERROR_CATEGORIES`.\n",
    "\n",
    "- **Tells the model to respond with only the category name**, with no explanations, extra text, or formatting.\n",
    "\n",
    "- **Keeps the formatting clean and clear**, so it's easy for the language model to follow.\n",
    "\n",
    "> Example of what you're aiming for (don‚Äôt copy this ‚Äî write your own version):\n",
    "> - A short paragraph or bullet list describing the error analysis task.\n",
    "> - Clearly labeled fields: \"PROBLEM\", \"CORRECT ANSWER\", \"PREDICTED ANSWER\".\n",
    "> - A final line instructing the model to choose **only one category** from the list and respond with **just the category name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eca27f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load error analysis model from utils\n",
    "ERROR_CATEGORIES = [\n",
    "    \"calculation_error\",   # Math computation mistakes  \n",
    "    \"reasoning_error\",     # Logical reasoning issues\n",
    "    \"incomplete_solution\", # Partial or unfinished answers\n",
    "    \"format_error\",       # Wrong answer format\n",
    "    \"other\"              # Other errors\n",
    "]\n",
    "\n",
    "# Initialize local model for error analysis\n",
    "tokenizer, model, hf_model_available = load_error_analysis_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af058f9a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 2\n",
    "def analyze_error(question: str, correct_answer: str, model_response: str, predicted_answer: str) -> Dict:\n",
    "    \"\"\"Analyze what type of error the model made using local Hugging Face model.\"\"\"\n",
    "    \n",
    "    # Try using local Hugging Face model if available (same prompt as GPT-4)\n",
    "    if hf_model_available:\n",
    "        try:\n",
    "            ### START CODE HERE ###\n",
    "            #üí° HINT: Create prompt for error analysis and categorizing the error. makes sure to include the question, correct answer, and predicted answer and ask for one of the ERROR_CATEGORIES and make sure that LLM only responds with category name\n",
    "            prompt = None\n",
    "            ### END CODE HERE ###\n",
    "            # Tokenize the prompt \n",
    "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.to(model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs, \n",
    "                    max_new_tokens=20,  \n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    num_return_sequences=1,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the response\n",
    "            generated_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "            category = generated_text.strip().lower()\n",
    "            \n",
    "            # Clean up the response and validate\n",
    "            if category in ERROR_CATEGORIES:\n",
    "                return {\"category\": category, \"description\": f\"HF model classified as {category}\"}\n",
    "            \n",
    "            # If exact match not found, try partial matching\n",
    "            for cat in ERROR_CATEGORIES:\n",
    "                if cat.replace(\"_\", \" \") in category or any(word in category for word in cat.split(\"_\")):\n",
    "                    return {\"category\": cat, \"description\": f\"HF model classified as {cat}\"}\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"HF model inference failed: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # Fallback to simple rules (same as before)\n",
    "    if len(model_response) < 50:\n",
    "        return {\"category\": \"incomplete_solution\", \"description\": \"Response too short\"}\n",
    "    elif predicted_answer == \"\":\n",
    "        return {\"category\": \"format_error\", \"description\": \"No answer extracted\"}\n",
    "    else:\n",
    "        # Check if it's close (likely calculation error)\n",
    "        try:\n",
    "            pred_num = float(predicted_answer)\n",
    "            correct_num = float(correct_answer)\n",
    "            if abs(pred_num - correct_num) / max(abs(correct_num), 1) < 0.001:\n",
    "                return {\"category\": \"calculation_error\", \"description\": \"Close but incorrect\"}\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {\"category\": \"reasoning_error\", \"description\": \"Logic error\"}\n",
    "# END GRADED SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9967b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "##  Error Clustering <a id=\"errorclustering\"></a>\n",
    "\n",
    "This section analyzes the incorrect predictions made by your model to identify common error patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366e82f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "New training data will be generated to help the model fix its most common mistakes. This targeted approach helps the model learn from its weaknesses and improves performance on challenging mathematical reasoning tasks.\n",
    "\n",
    "First, review the selected training examples. Then, proceed to fine-tune the model using this focused dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20c871",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Import caching utilities for error analysis\n",
    "from utils.utils import cached_analyze_errors, load_error_analysis_cache\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "# Get incorrect predictions\n",
    "incorrect_indices = [i for i, correct in enumerate(results['correct']) if not correct]\n",
    "\n",
    "# Prepare error data for analysis\n",
    "analysis_limit = min(300, len(incorrect_indices))\n",
    "print(f\"Analyzing {analysis_limit} incorrect predictions...\")\n",
    "\n",
    "error_questions = [results['questions'][idx] for idx in incorrect_indices[:analysis_limit]]\n",
    "error_correct_answers = [results['correct_answers'][idx] for idx in incorrect_indices[:analysis_limit]]\n",
    "error_model_responses = [results['model_responses'][idx] for idx in incorrect_indices[:analysis_limit]]\n",
    "error_predicted_answers = [results['predictions'][idx] for idx in incorrect_indices[:analysis_limit]]\n",
    "\n",
    "# Create cache key for error analysis\n",
    "error_data = f\"errors:{len(error_questions)}:{':'.join(error_questions[:3])}\"  # Sample for key\n",
    "cache_key = hashlib.md5(error_data.encode()).hexdigest()[:16]\n",
    "\n",
    "# Try to load from cache first - if exists, use it directly without calling analyze_error functions\n",
    "error_analyses = load_error_analysis_cache(cache_key)\n",
    "\n",
    "if error_analyses is not None:\n",
    "    print(\"‚úÖ Found cached error analysis - skipping LLM calls completely!\")\n",
    "else:\n",
    "    print(\"üîÑ No cache found - running fresh error analysis and caching results...\")\n",
    "    # Only call the expensive error analysis function if no cache exists\n",
    "    error_analyses = cached_analyze_errors(\n",
    "        analyze_error, \n",
    "        error_questions,\n",
    "        error_correct_answers, \n",
    "        error_model_responses, \n",
    "        error_predicted_answers\n",
    "    )\n",
    "\n",
    "# Add original indices back to the analyses for consistency with later code\n",
    "for i, analysis in enumerate(error_analyses):\n",
    "    analysis['example_index'] = incorrect_indices[i] if i < len(incorrect_indices) else i\n",
    "\n",
    "# Save results\n",
    "with open('lab_results/error_analysis.json', 'w') as f:\n",
    "    json.dump(error_analyses, f, indent=2)\n",
    "\n",
    "# Show error distribution\n",
    "error_categories = [analysis['category'] for analysis in error_analyses]\n",
    "category_counts = Counter(error_categories)\n",
    "\n",
    "print(\"\\nüìä Error Types Found:\")\n",
    "for category, count in category_counts.most_common():\n",
    "    percentage = (count / len(error_analyses)) * 100\n",
    "    print(f\"  {category}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d97f6b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Cluster the errors and get results\n",
    "embeddings, cluster_labels, embedding_model, kmeans, n_clusters = cluster_errors(error_analyses, results['questions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd56fd93",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The next cell will group similar errors and show key patterns. This helps you see which errors are most common and how they group together, making it easier to target improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a2a63",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Add cluster info to analyses\n",
    "for i, analysis in enumerate(error_analyses):\n",
    "    analysis['cluster'] = int(cluster_labels[i])\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_info = {}\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_errors = [a for a in error_analyses if a['cluster'] == cluster_id]\n",
    "    categories = [a['category'] for a in cluster_errors]\n",
    "    \n",
    "    cluster_info[cluster_id] = {\n",
    "        'size': len(cluster_errors),\n",
    "        'main_category': Counter(categories).most_common(1)[0][0],\n",
    "        'errors': cluster_errors\n",
    "    }\n",
    "\n",
    "    if cluster_errors:\n",
    "        example_idx = cluster_errors[0]['example_index']\n",
    "        print(f\"  Example: {results['questions'][example_idx][:60]}...\")\n",
    "\n",
    "# Simple visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 1. Error category pie chart\n",
    "plt.subplot(2, 2, 1)\n",
    "category_counts = Counter([a['category'] for a in error_analyses])\n",
    "plt.pie(category_counts.values(), labels=category_counts.keys(), autopct='%1.1f%%')\n",
    "plt.title('Error Types')\n",
    "\n",
    "# 2. Cluster sizes\n",
    "plt.subplot(2, 2, 2)\n",
    "cluster_sizes = [cluster_info[i]['size'] for i in range(n_clusters)]\n",
    "plt.bar(range(n_clusters), cluster_sizes)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.title('Cluster Sizes')\n",
    "\n",
    "# 3. PCA visualization\n",
    "plt.subplot(2, 2, 3)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='tab10')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Error Clusters (PCA)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save clustering results\n",
    "with open('lab_results/clusters.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'cluster_info': {str(k): {'size': v['size'], 'main_category': v['main_category']} \n",
    "                        for k, v in cluster_info.items()},\n",
    "        'n_clusters': n_clusters\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3d589",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Training Data Generation <a id=\"section_generate\"></a>\n",
    "Now you want to create some training examples to help the model fix the most common types of errors.\n",
    "\n",
    "You will use embedding-based similarity search to select relevant examples from the training set that closely match the error patterns identified in your analysis. This approach ensures that the fine-tuning data is targeted and effective for addressing specific weaknesses.\n",
    "\n",
    "- Group errors by category (calculation, reasoning, format, etc.)\n",
    "- Use sentence embeddings to find training questions similar to each error type\n",
    "- Select a balanced set of examples for each error category\n",
    "- Format the selected examples for model fine-tuning\n",
    "\n",
    "This targeted data generation will help the model learn from its mistakes and improve performance on challenging mathematical reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547b0f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 3: Generate Targeted Training Data\n",
    "\n",
    "The next code cell will create targeted training examples to address the specific error patterns you identified.\n",
    "\n",
    "Your task is\n",
    "- **Embedding Creation**: Generate vectors for training questions\n",
    "- **Similarity Calculation**: Compare error patterns with training examples\n",
    "- **Index Selection**: Choose most relevant training examples\n",
    "\n",
    "**CAREFUL! There are two graded parts in this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17917d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 3\n",
    "\n",
    "# Helper functions for creating training data from error analysis\n",
    "def create_training_example(question: str, answer: str) -> Dict:\n",
    "    \"\"\"Create a training example with question and full answer from train data.\"\"\"\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "class TrainingExampleSelector:\n",
    "    \"\"\"Class to select training examples similar to test error patterns using embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, error_analyses, train_data, embedding_model, samples_per_category=20):\n",
    "        self.error_analyses = error_analyses\n",
    "        self.train_data = train_data\n",
    "        self.embedding_model = embedding_model\n",
    "        self.samples_per_category = samples_per_category\n",
    "        self.error_by_category = {}\n",
    "        self.train_questions = None\n",
    "        self.train_embeddings = None\n",
    "        \n",
    "    def _group_errors_by_category(self):\n",
    "        \"\"\"Group errors by category for processing.\"\"\"\n",
    "        self.error_by_category = {}\n",
    "        for analysis in self.error_analyses:\n",
    "            category = analysis['category']\n",
    "            if category not in self.error_by_category:\n",
    "                self.error_by_category[category] = []\n",
    "            self.error_by_category[category].append(analysis)\n",
    "    \n",
    "    def _create_training_embeddings(self):\n",
    "        \"\"\"Create embeddings for all training questions.\"\"\"\n",
    "        print(\"üìä Creating embeddings for training data...\")\n",
    "    \n",
    "        ### START CODE HERE ###\n",
    "        # Create a list of questions from the training data.\n",
    "        # For each example in self.train_data you can access the question with ['question']\n",
    "        self.train_questions = None\n",
    "        # Use embedding_model.encode() on all training questions. Set the batch size to 32.\n",
    "        self.embedding_model.encode(None, batch_size=None, show_progress_bar=True)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    def _calculate_similarity_and_select_indices(self, error_questions):\n",
    "        \"\"\"Calculate similarity between error questions and training questions, return top indices.\"\"\"\n",
    "        error_embeddings = self.embedding_model.encode(error_questions)\n",
    "        ### START CODE HERE ###\n",
    "        #üí° HINT: Calculate similarity between error questions and training questions by using the cosine_similarity function.\n",
    "        similarity_matrix = None\n",
    "        ### END CODE HERE ### \n",
    "\n",
    "        # Get top similar training examples\n",
    "        similar_indices = set()\n",
    "        for i, error_question in enumerate(error_questions):\n",
    "\n",
    "            # Get top similar training examples for this error\n",
    "            similarities = similarity_matrix[i]\n",
    "            # Use np.argsort() to get indices of top similar training examples\n",
    "            top_indices = np.argsort(similarities)[-self.samples_per_category//len(error_questions):]\n",
    "            # Convert numpy int64 to regular Python int for HuggingFace dataset compatibility\n",
    "            similar_indices.update([int(idx) for idx in top_indices])\n",
    "             \n",
    "        return similar_indices, similarity_matrix\n",
    "    \n",
    "    def find_examples(self):\n",
    "        \"\"\"Public method to find training examples similar to test error patterns using embeddings.\"\"\"\n",
    "        \n",
    "        print(\"üîç Finding similar training examples using embedding similarity...\")\n",
    "        \n",
    "        # Step 1: Group errors by category\n",
    "        self._group_errors_by_category()\n",
    "        \n",
    "        # Step 2: Create embeddings for training data\n",
    "        self._create_training_embeddings()\n",
    "        \n",
    "        selected_training_examples = []\n",
    "        \n",
    "        # Step 3: Process each error category\n",
    "        for category, category_errors in self.error_by_category.items():\n",
    "            print(f\"\\nüéØ Processing {category} errors ({len(category_errors)} examples):\")\n",
    "            \n",
    "            # Create embeddings for error questions (from test set - for similarity only, not training)\n",
    "            error_questions = []\n",
    "            for error in category_errors:\n",
    "                idx = error['example_index']\n",
    "                error_questions.append(results['questions'][idx])\n",
    "            \n",
    "            if not error_questions:\n",
    "                continue\n",
    "\n",
    "            # Step 4: Calculate similarity and get indices\n",
    "            similar_indices, similarity_matrix = self._calculate_similarity_and_select_indices(error_questions)\n",
    "            \n",
    "            # Step 5: Create training examples from selected indices\n",
    "            category_examples = []\n",
    "            for train_idx in similar_indices:\n",
    "                question = self.train_data[train_idx]['question']\n",
    "                answer = self.train_data[train_idx]['answer']\n",
    "                category_examples.append(create_training_example(question, answer))\n",
    "            \n",
    "            selected_training_examples.extend(category_examples)\n",
    "            print(f\"   ‚Ä¢ Selected {len(category_examples)} similar training examples\")\n",
    "            \n",
    "            # Show example similarity\n",
    "            if category_examples:\n",
    "                best_idx = int(np.argmax(similarity_matrix[0]))  # Convert to Python int\n",
    "                similarity_score = similarity_matrix[0][best_idx]\n",
    "                print(f\"   ‚Ä¢ Best similarity score: {similarity_score:.3f}\")\n",
    "                print(f\"   ‚Ä¢ Example: {self.train_data[best_idx]['question'][:80]}...\")\n",
    "        \n",
    "        return selected_training_examples\n",
    "\n",
    "def find_similar_training_examples(error_analyses, train_data, embedding_model, samples_per_category=20):\n",
    "    \"\"\"Find training examples similar to test error patterns using embeddings.\"\"\"\n",
    "    selector = TrainingExampleSelector(error_analyses, train_data, embedding_model, samples_per_category)\n",
    "    return selector.find_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c5abb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "New training data will be generated to help the model fix its most common mistakes. This targeted approach helps the model learn from its weaknesses and improves performance on challenging mathematical reasoning tasks.\n",
    "\n",
    "First, review the selected training examples. Then, proceed to fine-tune the model using this focused dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463385f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Identify error patterns from your analysis\n",
    "print(\"Creating training examples from train set based on error patterns...\")\n",
    "\n",
    "# Get the most common error categories from your analysis\n",
    "if 'error_analyses' in locals() and len(error_analyses) > 0:\n",
    "    error_categories = [analysis['category'] for analysis in error_analyses]\n",
    "    category_counts = Counter(error_categories)\n",
    "    print(f\"Main error types found: {list(category_counts.keys())}\")\n",
    "    \n",
    "    # Use embedding similarity to find relevant training examples\n",
    "    if 'embedding_model' in locals():\n",
    "        selector = TrainingExampleSelector(error_analyses, train, embedding_model, samples_per_category=30)\n",
    "        training_examples = selector.find_examples()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Embedding model not available, falling back to random sampling...\")\n",
    "        train_indices = random.sample(range(len(train)), min(200, len(train)))\n",
    "        training_examples = []\n",
    "        for idx in train_indices:\n",
    "            question = train[idx]['question']\n",
    "            answer = train[idx]['answer']\n",
    "            training_examples.append(create_training_example(question, answer))\n",
    "else:\n",
    "    # Fallback: random sampling from train set\n",
    "    print(\"‚ö†Ô∏è No error analysis available, using random training sampling...\")\n",
    "    train_indices = random.sample(range(len(train)), min(200, len(train)))\n",
    "    training_examples = []\n",
    "    for idx in train_indices:\n",
    "        question = train[idx]['question']\n",
    "        answer = train[idx]['answer']\n",
    "        training_examples.append(create_training_example(question, answer))\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(training_examples)} targeted training examples from train set\")\n",
    "\n",
    "# Convert to dataset format  \n",
    "training_data = {\n",
    "    'question': [ex['question'] for ex in training_examples],\n",
    "    'answer': [ex['answer'] for ex in training_examples]\n",
    "}\n",
    "training_dataset = Dataset.from_dict(training_data)\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nüìù Sample Training Example:\")\n",
    "sample = training_examples[0]\n",
    "print(f\"Question: {sample['question'][:150]}...\")\n",
    "print(f\"Answer: {sample['answer'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b439732",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Model Fine-tuning <a id=\"section_tune\"></a>\n",
    "\n",
    "The next cells will format training data for the model, create training and validation sets, set up learning parameters and monitoring, run fine-tuning with progress tracking, and persist the improved model for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf329b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 4: Fine-tune Model with Targeted Data\n",
    "\n",
    "The tokenization function converts your training examples into the format expected by the model.\n",
    "\n",
    "Your task is to pass the `formatted_texts` to the tokenizer and assign it to the `model_inputs` variable. The tokenizer should also be passed the arguments `truncation=True`, `padding=True`, `max_length=512`, and `return_tensors=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bfefa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def format_training_examples(examples):\n",
    "    \"\"\"Format training examples into conversation format.\"\"\"\n",
    "    formatted_texts = []\n",
    "    for question, answer in zip(examples['question'], examples['answer']):\n",
    "        # Create a clear conversation format\n",
    "        formatted_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    return formatted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f42253",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 4\n",
    "\n",
    "def tokenize_and_format(examples):\n",
    "    \"\"\"Improved tokenization for stable training convergence.\"\"\"\n",
    "\n",
    "    # Format training examples using separate formatting function\n",
    "    formatted_texts = format_training_examples(examples)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use the tokenizer on formatted_texts.\n",
    "    # Set truncation and padding to True, max_length to 512 and return_tensors to None\n",
    "    model_inputs = None\n",
    "    ### END CODE HERE ###\n",
    "    # Create labels (copy of input_ids for causal LM)\n",
    "    model_inputs['labels'] = model_inputs['input_ids'].copy()\n",
    "    return model_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1af23",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Here, you split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1f419",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Split data and setup training with improved configuration\n",
    "train_size = int(0.8 * len(training_dataset))\n",
    "train_data = training_dataset.select(range(train_size))\n",
    "val_data = training_dataset.select(range(train_size, len(training_dataset)))\n",
    "\n",
    "print(f\"Training on {len(train_data)} examples\")\n",
    "print(f\"Validating on {len(val_data)} examples\")\n",
    "\n",
    "print(\"üîÑ Tokenizing training data...\")\n",
    "train_data = train_data.map(\n",
    "    tokenize_and_format, \n",
    "    batched=True, \n",
    "    remove_columns=train_data.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "print(\"üîÑ Tokenizing validation data...\")\n",
    "val_data = val_data.map(\n",
    "    tokenize_and_format, \n",
    "    batched=True, \n",
    "    remove_columns=val_data.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9a8bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Model Re-evaluation <a id=\"modelreevaluation\"></a>\n",
    "\n",
    "This section sets up the fine-tuning process with improved stability and monitoring:\n",
    "\n",
    "- `DataCollatorForLanguageModeling` is used for for efficient batching and padding, optimized for causal language modeling.\n",
    "- The HuggingFace `Trainer` is used to manage training, evaluation, and model saving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52855e2f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create improved data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal language modeling\n",
    "    pad_to_multiple_of=8,  # For efficiency on modern hardware\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Improved training arguments for stable convergence\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir='./training_logs',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=5, \n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy='steps', \n",
    "    eval_steps=5,  \n",
    "    \n",
    "    # Training parameters - more conservative for stability\n",
    "    learning_rate=1e-8,  \n",
    "    weight_decay=0.1,   \n",
    "    warmup_steps=20,    \n",
    "    max_grad_norm=1.0,   # Gradient clipping for stability\n",
    "    \n",
    "    # Batch sizes - smaller for stability\n",
    "    per_device_train_batch_size=1,   \n",
    "    per_device_eval_batch_size=2,    \n",
    "    gradient_accumulation_steps=4,   \n",
    "    \n",
    "    # Epochs\n",
    "    num_train_epochs=2,  \n",
    "    \n",
    "    # Disable unnecessary features\n",
    "    report_to=[],\n",
    "    save_strategy=\"no\",  # Don't save checkpoints during training\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "try:\n",
    "    training_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n‚úÖ Fine-tuning complete!\")\n",
    "    print(f\"üìà Final training loss: {training_result.training_loss:.4f}\")\n",
    "    \n",
    "    # Print validation loss progression if available\n",
    "    log_history = trainer.state.log_history\n",
    "    eval_losses = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry and not (entry['eval_loss'] != entry['eval_loss'])]  # Filter out NaN\n",
    "    # Check for training issues\n",
    "    train_losses = [entry['loss'] for entry in log_history if 'loss' in entry and not (entry['loss'] != entry['loss'])]  \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Fine-tuning failed: {e}\")\n",
    "    print(\"Continuing with original model for demonstration...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edcf524",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The next code cell will visualize the training and validation loss curves from the fine-tuning process. This helps you monitor convergence detect overfitting, and assess training stability.\n",
    "\n",
    "Review the plot to ensure your model is improving and training is stable before moving on to evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717a2ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plot training vs validation loss (from solutions.ipynb)\n",
    "def plot_train_vs_val(log_history):\n",
    "    \"\"\"Plot training vs validation loss to check for overfitting.\"\"\"\n",
    "    # Separate training and evaluation logs\n",
    "    train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "    eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "    if len(train_logs) == 0 or len(eval_logs) == 0:\n",
    "        print(\"‚ö†Ô∏è Insufficient training/validation data for plotting\")\n",
    "        return\n",
    "\n",
    "    # Extract values\n",
    "    train_steps = [log['step'] for log in train_logs]\n",
    "    train_losses = [log['loss'] for log in train_logs]\n",
    "\n",
    "    eval_steps = [log['step'] for log in eval_logs]\n",
    "    eval_losses = [log['eval_loss'] for log in eval_logs]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_steps, train_losses, label='Training Loss', marker='o', alpha=0.7)\n",
    "    plt.plot(eval_steps, eval_losses, label='Validation Loss', marker='s', alpha=0.7)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Print actual values for analysis\n",
    "    print(f\"Training losses: {train_losses[-5:]}\")  # Last 5 values\n",
    "    print(f\"Validation losses: {eval_losses[-3:]}\")  # Last 3 values\n",
    "\n",
    "# Visualize training progress if trainer is available\n",
    "if 'trainer' in locals() and hasattr(trainer, 'state'):\n",
    "    print(\"üìä Training Loss Analysis:\")\n",
    "    plot_train_vs_val(trainer.state.log_history)\n",
    "\n",
    "\n",
    "\n",
    "# Use the fine-tuned model directly from the trainer (already in memory)\n",
    "if 'trainer' in locals() and hasattr(trainer, 'model'):\n",
    "    fine_tuned_model = trainer.model\n",
    "    fine_tuned_tokenizer = tokenizer  # Same tokenizer used for training\n",
    "    fine_tuned_model.eval()\n",
    "    print(\"‚úÖ Using fine-tuned model from training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No fine-tuned model available\")\n",
    "    print(\"Using original model for demonstration...\")\n",
    "    fine_tuned_model = model\n",
    "    fine_tuned_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7859d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Compare your fine-tuned model to the original model to see how much it improved.\n",
    "\n",
    "You'll continue through the flywheel:\n",
    "\n",
    "1. Evaluate again\n",
    "\n",
    "2. Analyze errors\n",
    "\n",
    "3. Fix what you can\n",
    "\n",
    "4. Retrain with improvements (if needed)\n",
    "\n",
    "This loop ensures your model is always evolving ‚Äî carefully and responsibly.\n",
    "\n",
    "> **Note**: It should take no more than 7 minutes to run this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818172d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the fine-tuned model\n",
    "improved_results = evaluate_model(fine_tuned_model, fine_tuned_tokenizer, eval_dataset, \"Fine-tuned Model\")\n",
    "\n",
    "# Save results\n",
    "with open('lab_results/improved_evaluation_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'accuracy': improved_results['accuracy'],\n",
    "        'total_examples': len(improved_results['correct']),\n",
    "        'correct_predictions': sum(improved_results['correct'])\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ac23d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "Below are sample questions where the fine-tuned model corrected errors made by the original model:\n",
    "\n",
    "- Question: Shows the math problem.\n",
    "- Correct Answer: The ground truth solution.\n",
    "- Original Prediction: The answer given by the baseline model.\n",
    "- Improved Prediction: The answer given by the fine-tuned model.\n",
    "\n",
    "If no improvements are found, a message will indicate that in this sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097061ce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "print(f\"Original Model Accuracy:   {results['accuracy']:.3f}\")\n",
    "print(f\"Fine-tuned Model Accuracy: {improved_results['accuracy']:.3f}\")\n",
    "improvement = improved_results['accuracy'] - results['accuracy']\n",
    "print(f\"Improvement:               {improvement:+.3f}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    percentage_improvement = (improvement / results['accuracy']) * 100\n",
    "    print(f\"Relative Improvement:      {percentage_improvement:.1f}%\")\n",
    "    print(\"üéâ The fine-tuned model performs better!\")\n",
    "else:\n",
    "    print(\"ü§î The fine-tuned model didn't improve (this can happen with small datasets)\")\n",
    "\n",
    "# Show some examples where the model improved\n",
    "print(\"\\nüîç Examples where the fine-tuned model improved:\")\n",
    "improvement_count = 0\n",
    "for i in range(len(eval_dataset)):\n",
    "    original_correct = results['correct'][i]\n",
    "    improved_correct = improved_results['correct'][i]\n",
    "    \n",
    "    if not original_correct and improved_correct and improvement_count < 3:\n",
    "        improvement_count += 1\n",
    "        print(f\"\\n--- Improvement Example {improvement_count} ---\")\n",
    "        print(f\"Question: {results['questions'][i][:100]}...\")\n",
    "        print(f\"Correct Answer: {results['correct_answers'][i]}\")\n",
    "        print(f\"Original Prediction: {results['predictions'][i]}\")\n",
    "        print(f\"Improved Prediction: {improved_results['predictions'][i]}\")\n",
    "\n",
    "if improvement_count == 0:\n",
    "    print(\"   No clear improvements found in this sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b681548",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Analyze the improvement in error clusters, visualize before/after performance, and generate comprehensive evaluation reports with statistical significance tests.\n",
    "\n",
    "This section shows a simple bar chart comparing the accuracy of the original and fine-tuned models.\n",
    "\n",
    "- **Blue Bar**: Original model accuracy\n",
    "- **Green Bar**: Fine-tuned model accuracy\n",
    "- **Value Labels**: Exact accuracy shown above each bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc44fb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Model Accuracy Comparison <a id=\"modelaccuracycomparison\"></a>\n",
    "\n",
    "This section shows a simple bar chart comparing the accuracy of the original and fine-tuned models.\n",
    "\n",
    "Use this chart for a quick visual check of model improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ef7fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "models = ['Original', 'Fine-tuned']\n",
    "accuracies = [results['accuracy'], improved_results['accuracy']]\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "bars = plt.bar(models, accuracies, color=colors, alpha=0.7)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507904b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This section shows how model predictions changed after fine-tuning.\n",
    "\n",
    "Change Types:\n",
    "- **Still Wrong**: Incorrect before and after\n",
    "- **Still Correct**: Correct before and after\n",
    "- **Newly Correct**: Fixed by fine-tuning\n",
    "- **Newly Wrong**: Became incorrect after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a95d07",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# 2. Improvement breakdown\n",
    "plt.figure(figsize=(6, 6))\n",
    "improvement_data = {\n",
    "    'Still Wrong': sum(1 for o, i in zip(results['correct'], improved_results['correct']) if not o and not i),\n",
    "    'Still Correct': sum(1 for o, i in zip(results['correct'], improved_results['correct']) if o and i),\n",
    "    'Newly Correct': sum(1 for o, i in zip(results['correct'], improved_results['correct']) if not o and i),\n",
    "    'Newly Wrong': sum(1 for o, i in zip(results['correct'], improved_results['correct']) if o and not i)\n",
    "}\n",
    "\n",
    "colors = ['red', 'green', 'lightgreen', 'orange']\n",
    "plt.bar(range(len(improvement_data)), list(improvement_data.values()), \n",
    "        color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(improvement_data)), list(improvement_data.keys()), rotation=45)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975d80d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Final Results Summary <a id=\"finalresultssummary\"></a>\n",
    "\n",
    "This section gives a quick overview of your model improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b47f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original Model Accuracy:    {results['accuracy']:.4f}\")\n",
    "print(f\"Fine-tuned Model Accuracy:  {improved_results['accuracy']:.4f}\")\n",
    "\n",
    "improvement = improved_results['accuracy'] - results['accuracy']\n",
    "print(f\"Absolute Improvement:       {improvement:+.4f}\")\n",
    "\n",
    "if results['accuracy'] > 0:\n",
    "    relative_improvement = (improvement / results['accuracy']) * 100\n",
    "    print(f\"Relative Improvement:       {relative_improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nCorrect Predictions:\")\n",
    "print(f\"  Original:    {sum(results['correct'])}/{len(results['correct'])}\")\n",
    "print(f\"  Fine-tuned:  {sum(improved_results['correct'])}/{len(improved_results['correct'])}\")\n",
    "print(f\"  Net Gain:    {sum(improved_results['correct']) - sum(results['correct'])}\")\n"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
