Data is one of the most important things in post-training.
0:04
You probably already got that sense from evals, because as you look at those errors in your evaluation,
0:11
all of your fixes are often in the data space.
0:14
So one question I often get asked is, how much data will help me fix my problem?
0:19
Let's take a look.
0:21
So how much data you need in post-training depends a ton on your pre-trained model that you're post-training on.
0:27
I found it's also helpful to compare these two stages and how data plays in.
0:31
For pre-training, it really is all about scale.
0:34
You need a lot of data because the model is trying to learn so many different things across a general set of tasks.
0:41
So how much data you need in post-training depends a ton on your pre-trained model that you're post-training on.
0:47
And in pre-training, it was all about scale.
0:49
Your pre-trained model has learned on a ton of tokens in order to learn general-purpose tasks.
0:55
From the DeepMind paper on the Chinchilla model in 2022, they found that you need 20 tokens per parameter for a model.
1:03
And that might mean a 10-billion-parameter model should ideally be trained on 200 billion tokens.
1:08
That's so many tokens on the internet.
1:11
And if you look at GPT-3, it was pre-trained on about 300 billion tokens.
1:15
So the scale of things is just massive in pre-training.
1:18
That rule isn't hard and fast, but it's a good anchoring point to understand how much data pre-training can really handle to efficiently learn.
1:24
Post-training is really different.
1:26
It's less about the scale of data at that scale.
1:29
More data still does help, but it really is about the quality of that data to reshape the behavior of the model.
1:36
For example, ChatGPT used about 13,000 fine-tuning examples, way fewer than those billions of tokens,
1:43
which might amount to hundreds of thousands or still billions of examples,
1:47
and thousands of preference pairs, less than a million certainly, for their reward model.
1:52
So diving in a bit, when are 50 examples enough versus 100,000 examples?
1:57
When can you get away with just a few examples, and when do you need a ton?
2:01
Well, if your pre-trained model, for example, already knows calculus, already knows math,
2:06
and your goal with post-training is to just learn a new calculus exam format,
2:10
you might only need 20 practice problems for the model to learn a little bit about that new exam format.
2:16
But if the model had never seen calculus before and it needs to learn this new calculus exam format,
2:22
that 20 examples is probably not enough because you probably also have to teach it an entire calculus course,
2:28
and that's looking at more like thousands of examples.
2:31
Next, if your model has never seen math at all, then this goal of getting to this calculus exam is going to be much harder,
2:38
and you're going to need a lot more examples to demonstrate those math courses.
2:42
So basically, you have to look at what your pre-trained model already knows for your task in post-training,
2:49
and if it's not in pre-training, then you can't actually recreate it cheaply in post-training.
2:54
So always run evals first on your pre-trained model to understand what your model already knows,
3:00
what its capabilities already are, and that'll help you estimate the amount of data you need for your task in post-training.
3:06
So as an example, if you're looking at these evals, maybe your model knows addition already, pretty good at multiplication,
3:12
but it hasn't really mastered division or mixed expressions.
3:15
Then you can actually see what your interventions need to be and what kind of data you need to teach the model basic math in this case.
3:23
And of course, you want to start with a minimal amount of data.
3:26
You want to always start with the smallest amount of data so that you can draw a conclusion on what type of data you need before you scale it out.
3:33
So before you actually scale out to the exact amount of data you even need, you want to start even smaller than that,
3:40
because your hypothesis might not be correct.
3:42
Then I would think about scaling until you reach diminishing returns.
3:46
So maybe you start with 20 examples. That's probably enough to see small changes in formatting.
3:52
On the order of hundreds, you'll start to see some model changes.
3:56
On the order of thousands is typically one of those sweet spots of you'll see a lot of tasks be able to be achieved on the order of thousands.
4:04
Tens of thousands is probably looking at post-training for a lot of the frontier labs.
4:08
And then hundreds of thousands is you're adding new domain knowledge.
4:12
LoRAs can also help you get away with less data because you're changing fewer weights of the model,
4:17
and you can actually adapt it to that targeted dataset much more effectively.
4:22
For LoRA, you really need to think about the rank.
4:24
You want to be able to get away with as low of a rank as possible because that is tuning fewer parameters.
4:29
So with smaller sets of data, you can get away with really small rank.
4:33
And then, of course, with larger sets of data, you may have to increase that rank to actually see changes in the model.
4:38
For preference learning, so to train your reward model, you can see similar types of dataset sizes.
4:43
So empirically, what that looks like is on the order of thousands of examples, you can see improvement in your reward model.
4:49
For more nuanced distinctions, you might want more data showing like the fine-grained distinction between A and B.
4:55
But the reward model is closer almost to a classifier.
4:57
You're trying to understand what the line is between a certain type of preference.
5:01
And it can be continually trained during RL training, which you'll see a little bit more later, called online preference learning.
5:07
And the outputs don't necessarily have to be generated by the LLM that it's grading over here.
5:11
So it could be written out by a person.
5:13
It could be generated by a different language model or a different type of model.
5:18
And that actually helps the reward model not overfit to your existing model.
5:22
Ultimately, in RL training, it will depend heavily on your reward model's capabilities.
5:27
And in improving your reward model, you can actually see gains in your resulting LLM trained with that reward model.
5:34
Of course, you will see saturation in that as well.
5:37
So as you 10x the dataset size for your reward model, you may see saturated performance.
5:42
And that is when you know you've tapered off and you don't need to improve your reward model anymore.
5:46
Because, ultimately, the goal of your reward model is to improve this final LLM.
5:51
The performance of your model in RL also depends on the input distribution.
5:55
So just like in fine-tuning, where you really need to have diverse examples, including in the input, you also need that here in RL.
6:02
You need very diverse input to actually cover your task and represent your task effectively.
6:07
Every input can also have multiple outputs.
6:10
The default in Hugging Face is 8.
6:12
And that can actually help you estimate the reward variance.
6:15
So basically, you can have many, many rollouts coming from a single input, but multiple outputs.
6:20
And in state-of-the-art runs today, we're looking at hundreds of thousands to maybe even a million rollouts.
6:26
That's really helping the model understand kind of the variance of given multiple types of outputs it could produce, how well can it do?
6:34
So you saw fine-tuning, preference learning, and rollouts.
6:37
You saw kind of the scale of things.
6:39
Here's just a bit of a diagram to show and compare them.
6:43
And as you can see, you do see some saturation as you increase the number of examples.
6:47
Ultimately, all of this is empirical.
6:49
And what you want to do is test things out at a very small scale before you increase things over time.
6:55
For post-training, probably the most important thing is not just more data, but careful data.
7:01
So just having a few curated examples is going to always beat the noisy examples.
7:06
That's going to be different than in pre-training.
7:08
Basically, if you have the choice between 100 curated examples versus 10,000 noisy, maybe synthetically generated examples,
7:17
you should always pick the 100 because that will actually get you the better ultimate model in the end.
7:23
And this is just showing an AI Marie Kondo who prefers the curated examples over the noisy, messy ones.
7:30
Now that you understand how much data you need for post-training, let's hone in on fine-tuning data first.
