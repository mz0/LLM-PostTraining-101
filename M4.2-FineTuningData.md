Data is one of the most important pieces of fine-tuning. Take a look at how to iteratively
0:05
add targeted examples in the fine-tuning flow. So as a bit of a refresher, fine-tuning,
0:11
you need input data and target output data pairs. And for reasoning, you need the output pairs to
0:17
have kind of that thinking with an answer. What that looks like is here's a possible input where,
0:23
you know, Alice has three apples, buys two more, how many now, with a target output of this
0:27
rationale and an answer. For reasoning, you need those think tags. Now, going into a little bit
0:32
more customization, if you want to have custom prompt tags, this is a good opportunity to actually
0:38
teach your model that. For example, you maybe are hitting an API in the input and you always include
0:43
today's weather in some way. So you have that tag in your input. And so the model will learn
0:48
to understand that tag as input and produce the right output based on it. And maybe in the output,
0:53
you also want to do the same thing. Maybe you want to hit an API or you want to, in this case,
0:57
display the weather. So you want the model to output some kind of emoji to be displayed
1:02
on a website. And this output could then be programmatically extracted if you learn these
1:06
new custom tags. So let's take a look at what that looks like iteratively as you refine the
1:11
model and improve it. So your goal is to teach the model new custom tags as a format. And maybe you
1:17
start with 20 examples just to see the model change a little bit. And you use LoRA fine-tuning to do
1:22
that. You evaluate the model and you do some error analysis and you find the worst failure pattern
1:28
and you target that. And the worst one you see is it omits that display weather tag. And you don't
1:34
want that, right? You want the model to actually show that so you can display it on your website.
1:38
But you see that it's not always the case that it omits it. It's usually right. But when the word
1:44
sunny is next to some punctuation mark in the input, then you see that problem occur. So this
1:49
is just to give you a sense of the type of error pattern that might occur. And it might not be super
1:54
obvious, right? You might have to dig through a lot of examples to see this pattern and have this
1:59
hypothesis of why this error is happening. Okay, so maybe you figured out that that is the error.
2:04
Your data fix here is to add a few examples that kind of counter this and teach the model
2:10
this pattern. So maybe you could vary the punctuation a bit, add some synonyms like
2:14
bright sunshine. With punctuation, of course, you can vary the capitalization a bit. And these
2:19
examples will help the model understand, okay, whenever there is sunny with punctuation mark,
2:24
and I now see more examples of that, I will output the display weather thing appropriately.
2:28
And of course, you saw those examples with the same input. You will want to vary that input as
2:33
much as you can so that the model sees a broad variety of math problems here. So then the next
2:38
iteration is you do some more fine-tuning with additional examples mixed in. Great, that was
2:43
fixed. You test some of that. That's in your evals. Next, you do the same evaluation. You do
2:48
error analysis again, and you find the worst failure pattern. Here, you see maybe it uses the
2:53
wrong emoji for clear skies. It shows a cloud instead of a sun emoji. And then you can introduce
3:00
a few counterexamples of that. Maybe the next iteration, you see there's no, when you don't
3:05
give an API tag, the model still is a bit leaky and will still use those think tokens and display
3:10
weather. And maybe you don't want the model to actually do that when a user is asking a math
3:14
problem without that API hit. And so you want to add this no tag example so it doesn't leak its
3:20
thoughts. Finally, maybe the model is not adhering to the schema correctly. And this is where you can
3:26
add really strict schema adherence examples. So this is a very iterative process. As you can see,
3:31
the data created is very targeted to your failure patterns. Now, one question is, when do I stop?
3:37
I could probably keep going, keep going, keep going. And that's one of the beauties, but also
3:41
challenges of AI is that it's never going to be perfect. So you could actually continue going.
3:46
But essentially, use your evals as a guide for when a model is good enough or better than your
3:51
production model today. And you're able to release it and actually collect some understanding of what
3:57
users will actually use it for. Some considerations for LoRAs are where do you want to actually put
4:03
the LoRAs? Where you add the LoRAs can depend on the type of task you want to do and also how many
4:08
LoRA adapters you want to learn. Often, you want to put it on attention. That's the typical way.
4:12
If you want to put LoRAs on your multilayer perceptron, your linear layers as well,
4:17
that can help with stronger fact recall in the model. And then the rank of the LoRAs matters
4:21
a lot. When you have a few examples, you can get away with the lower rank. And when you have many
4:25
examples, you may need higher rank for fine-tuning and actually getting the behavior to change.
4:30
For LoRAs, often, you'll see more steps are preferred with smaller batch sizes to actually
4:35
maximize your update diversity. I would always recommend starting with LoRA fine-tuning.
4:40
Over time, as you look at your task and as your dataset increases, you might be looking at
4:45
something that is a broader knowledge shift. And that is where you need to think about full fine-
4:49
tuning. If you find that your LoRAs, even as you increase rank and increase the number of adapters,
4:54
are still not changing the model's behavior in the way you want it to, you want to revisit the data.
4:59
But you might also think about a full fine-tuning as well. This is catastrophic adherence.
5:04
Basically, the model doesn't really change when you train it. Another question is how much chain
5:08
of thought data? You've learned about reasoning a lot. How much should you actually be mixing in?
5:13
And these reasoning tags, these think tags, are actually a great way to teach the model
5:17
to reason and to get the model to think for better outputs. But what kind of mixture is
5:22
appropriate? Because you might not always want the model to be displaying that.
5:26
So, one of your guides is, what is the user going to use at inference time? How do you expect
5:32
the user to use your model? And at inference time, maybe the needs are, sometimes the user wants to
5:39
use reasoning, and they're checking off a box. For example, in ChatGPT, there was a chance where
5:43
you could check off a box to use thinking versus not. In which case, you may want the model to
5:49
actually see both types of inputs, with and without that use thinking, and to respond accordingly.
5:54
So that at inference time, when the user is using your model, you can switch dynamically between
5:59
them as a user's cue, here with the "use thinking" tag. You can, of course, make it more explicit.
6:04
You can say, "please use think", for example, and switch between them. Another consideration is
6:09
thinking through not just the user experience here, but more so what types of tasks the user
6:14
will need to use your model for. And if you're reasoning heavy tasks, I'd suggest actually
6:19
heavily shifting your dataset to be more reasoning, or at least 40% reasoning, and the rest
6:26
just regular direct answer, versus if you have latency-sensitive tasks, you need the model to
6:31
respond very efficiently. Then maybe you want more direct answer examples, so the model is actually
6:36
responding very quickly, as opposed to thinking for a long time.
