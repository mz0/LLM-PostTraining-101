0:02
Time to look at all the datasets that go into a production post-training pipeline.
0:06
So let's take a look at this for a frontier lab that came out earlier this year.
0:11
And the focus will be here on the data pipeline.
0:13
And you'll put together the models in the next module on a full production post-training pipeline.
0:17
So here's every single dataset here, all the datasets that you'll go over,
0:21
starting with the cold start long chain of thought data.
0:25
Exactly what does that mean?
0:26
So chain of thought data, you know, is reasoning data.
0:29
So it looks like this.
0:31
Long chain of thought just means that it's just really, really long reasoning.
0:35
So a lot of thinking in that thinking tag.
0:37
And then K-examples is referring to probably a small seed dataset of these and used for cold start.
0:43
So basically, this is just a cold start set, a small set to get started with fine-tuning
0:48
and getting the model used to this type of format.
0:51
Okay, so we got that.
0:52
What's next?
0:53
All right, the DeepSeq v3 fine-tuning data.
0:56
Let's take a look at that.
0:57
So this data doesn't involve any explicit chain of thought or reasoning.
1:02
So it's basically just regular input and target output.
1:05
Next is the reasoning data.
1:07
What does that look like?
1:08
So this is actually synthetic reasoning data.
1:11
So just a lot of generated reasoning data.
1:14
And specifically, you're generating a ton of data here and then filtering.
1:17
And it's probably important to go through the different filtering steps.
1:20
So one is rejection sampling.
1:22
So generating a lot of possible options and only keeping the top one, for example.
1:27
And then removing mixed languages.
1:29
This is something that's really easy to filter for when they see, oh, hey, in the dataset, there's English and Chinese.
1:36
Let's just remove that data point because we don't want to encourage that.
1:39
And then finally, removing code.
1:40
The model should not be answering with code, at least for this dataset and these inputs, these prompts.
1:45
And it doesn't make sense here.
1:47
And likely, the model was just overeager with generating code.
1:49
So this was an easy way to filter out that data.
1:52
So now we have a good set of reasoning data.
1:55
And note that that is 600k samples, so quite a bit there.
1:59
How is the non-reasoning data put together?
2:01
So the non-reasoning data is interesting.
2:04
So it's going to be sets of data points that might have some chain of thought, the paper mentions.
2:10
But probably using the target output as the input-output pair.
2:14
And then also an input-target-output pair, just like normal, where it doesn't have any chain of thought.
2:19
Cool.
2:20
So that's what that looks like.
2:21
And then you combine all of that together for the final dataset.
2:24
And that is what this looks like for the combined fine-tuning data of 800,000 samples.
2:29
About a quarter are non-reasoning.
2:31
And then 75% are reasoning.
2:34
And great.
2:35
That gets us all of the different datasets.
2:37
And that goes into this full pipeline.
2:39
So as you can see, there are a ton of possible different datasets.
2:42
It's not just kind of two datasets and that's it.
2:44
There are several, and they feed into different parts of the pipeline in terms of both the fine-tuning pieces and RL pieces.
2:51
Now that you know what kind of data you need, it's time to take a look at how you can use LLMs to generate a lot of that data for you.
