One effective strategy for scaling out synthetic data is using templates for your language models

to generate that data in a diverse but constrained way. So far, for the most part, you've been

looking at every single data at the sample level, so every single data point. Templates are a way

to scale that out, so be able to operate on a template, a prompt template, to generate data,

for example, instead of individual data examples. And this is really, really helpful for just

scaling things out more effectively, so that you can actually edit templates to correct failures

and not just individual data samples. Let's take a look at a template first. So here is an example

template. So it could just be, you know, hello, customer name. I understand you're having an issue

with product name, where it's specific issue. I've taken the action to resolution step. And then you

have a bunch of possible inputs, essentially mad-libbing those in, so you can just mix and

match those, and you can get some examples that you see down here. So this is just an example

template to understand how powerful this really is. Let's take a look at how templates can help us

more systematically organize the creation of training data. Okay, so you've trained your model,

and you see a failure pattern in error analysis. So the model gives, like, flat, one-sided answers.

Here, you can now create a template to generate natural contrast pairs. So your template might

be explain concept by comparing with contrast in a certain number of sentences. And then here's

an example of a possible input-output there. And so now you can create this great input-output pair

to use for your dataset, and you can create many, many of those. For enforcing diversity at the

template level, it could look like, okay, the model is giving only short answers, not multiple. So you

can create a template to actually generate those diverse examples. So you can list a certain number

of items, each with a certain requirement, to get that diversity. And here is an example input-output

here. What I found to be really interesting is that there's also ways to get diversity by telling

the model and, you know, going through a template and telling the model to go through and iterate

through various personas. And you can iterate through millions of personas that would generate

different types of output. Okay, so if the model is outputting unsafe completions, like in

Constitutional AI, they create this template to generate safer examples. So here's your request,

and if the request is safe, answer it. Otherwise, refuse politely and explain why. And this really

hones the model into focusing on this particular issue. So here's an example.

I've also found it to be fairly interesting to mix and match things using a template. So what

this means is just the model might be answering things in isolation from one database or another

or one dataset or another. But instead, if you want the model to be able to actually reason and

answer things across databases or across datasets, this is a way of getting that data in there. So

what is concept in database one and why is it important in domain and database two? And so

here's an example. And so that helps mix kind of the information together and actually teach the

model some examples that are at that intersection. So what could this look like in production? So

you might get human feedback when your model is used in production, and it might be informal,

maybe consistent. The user might say, like, this answer feels off or it's too wordy. And

what you can do is use the template to transform those subjective and possibly noisy, vague comments

into more structured signals. And so you can have this feedback and then you can convert that

informal feedback into a structured form. And then you have a great set of examples that actually

match that structure that you can then use. In your code, this is what it could look like. So

here's a double checking template to actually check, hey, is this solution correct? And can

you verify it with another method? So in production, what it looks like is the model, you release it,

and then you get some feedback from people. Maybe it's a limited release, but you get some feedback.

You can then operate at the template level. You can create a template from that feedback,

whether it is targeting issues that the feedback hones in on, or it's even just organizing the

feedback itself. And that creates then data for you. And then that data trains that new model.

So ultimately, operating at the template level helps you scale out and get quality, diverse data

pretty quickly in a structured, more constrained way. And then it enables you to edit these

templates instead. So you're not looking at thousands of data samples on their own,

but maybe just hundreds of templates. And then, of course, it fits into this production data flywheel

where you can actually edit the template when you get that user feedback back in.

Now that you know how to operate at the template level for synthetic data generation,
let's revisit Constitutional AI.
