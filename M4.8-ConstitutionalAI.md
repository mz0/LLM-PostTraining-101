
0:00
You've come a long way since you last took a close look at Constitutional AI.
0:04
Now, let's revisit it again through the lens of template engineering.
0:08
Cool. So, a quick revisit to Constitutional AI. In Constitutional AI, you have a person writing
0:14
a constitution. Then you have a model generating these input-output pairs. You have another model
0:19
critiquing outputs. This is where one of the templates come in to critique it. Then you pass
0:23
all this information in that's filtered, right, to fine-tuning. You get a fine-tuned model.
0:28
That fine-tuned model can actually generate different outputs. It can generate two outputs,
0:32
for example, or more for every input. Then another model using a template can select
0:36
better outputs. So, which one is better, right? Give that preference data to a reward model.
0:42
Then you can train a reward model based on that preference information to get an input,
0:46
a model output, and a reward, which then feeds into the RL training pipeline and gets you that
0:52
final aligned model, the model that's aligned to the constitution that the human first wrote.
0:57
Why you want to start with a person writing out that constitution is, again, because people are
1:01
really good at defining these general rules, but maybe not every single step of these equations,
1:06
especially at scale. But a person is really good at actually writing out, hey, these are the core
1:12
principles and rules that we need to enforce for the model. The model can take that, and many
different templates can take that and actually be able to transform that data to better formats
for then downstream training. This makes it so you don't have to manually label. People don't
have to manually label every single output on the model based on that rubric. You can just rely on AI.
You can rely on LLMs to scale that constitution to enforce those rules.

So, what does that look like? First, you can have a potentially harmful request coming in,

and this is where you can have a template. This template here is built around the principles
in the constitution. You can think of that as high-level rules, like avoiding harm or
respecting privacy. Instead of needing to manually label every single harmful request that comes in,
you can give the model a reusable structure here with this template, and it learns this pattern
once and then applies it everywhere. So, here the model first refuses the harmful request,
and then it names a principle being violated, like privacy or safety, and then it offers a safe
and helpful alternative. So, here is an example. So, I can't help with that because it violates
this principle, and that principle is coming from that constitution, and then it gives an
alternative of what it should have responded with instead. So, as you can see here, there's a
harmful request coming in. The AI basically sets a clear boundary, and then notice that the AI
doesn't just stop there. It goes one step further and offers a safe, useful alternative, and the
simple pattern shows basically two things. The system enforces those safety rules, catches them
in one go, and then on the other go, it actually is able to produce a constructive path. So, the
cool part is that, maybe for the fine-tuning step, the templates actually don't just guide
how the AI phrases its answers, they can actually become rewards, and reliable ones that follow the
constitution.

Here's how it works. When the AI gives a response, you can check, did it follow
the template? Did it refuse a harmful request, name the principle, and redirect safely? And if yes,
the response gets a higher score, and if not, it scores lower. And after that, RL can take over
based on the rewards, the scores that it got, and the models then train to optimize for those higher
scores. So, templates here can be more than just guidelines. They can turn into essentially reward
functions, right? And they can transform these human values like safety and respect into a
numeric signal that the algorithm can directly improve on. And so, that's how we bridge the gap
between human principles getting translated into machine rewards. So, to make the scoring really
fair and consistent, you can use a simple rubric. Here, you're looking at four different things. One
is helpfulness. Did we offer appropriate help in this output? Harmlessness. Did we avoid harm?
Honesty. Is it accurate and transparent? Autonomy. Do we respect user agency? There's no manipulation
of what the user is saying, and we're not manipulating them. And each of these can get a
score from 0 to 10, and then we have a weighted average to create an overall constitutional score.
So, this way, instead of scoring randomly or inconsistently or just generally, how is this
going? You have a structured way to measure if the model is being helpful, harmless, honest,
and respectful of the user. And when you have that score, you can then use it to train your models to
consistently aim for safer, higher-quality behavior.

Now, putting the rubric into action, here's an example. So, same input.
How do I hack someone's email? And look at the different responses.
Look at response A. The model actually tries to help with that hacking. That's a problem. Maybe
it's trying to be really helpful, but its score is very low because it fails on harmlessness and
it fails on respecting autonomy. Then we look at response B. This one follows a template. It
refuses the harmful request. It names the principle of respecting privacy and redirects to something
safe and constructive, like recovering your own account or learning better security.
So, now the difference between scores, response A gets a much lower score than response B
across the board. And this shows how the rubric and the template kind of work hand-in-hand
and provides a consistent way to measure these responses. And just as importantly,
give the model a clear learning cycle. This is the kind of answer we want more of.

Another way templates can help is when we're collecting preference data. So, these templates
or rubrics are also possibly good for humans as well. But here, how it works is you can show
to annotators these responses or you can show the model these two possible responses and ask
a simple question, what is better based on the rubric instead of either personal opinion for
the human case or just a general output from the model's case. And you want to understand kind of
why it arrived at that final score. So, the key point is that the templates will unify judgment.

So, it can unify it across both human responses and here automated responses. And that is really
helpful because that can reduce disagreement and noise. And therefore, the feedback is less messy
and more consistent. Zooming back out, you have that written constitution by the person,
but then also those templates for critiquing the outputs and selecting a better output
in this overall pipeline for Constitutional AI.
