One of the key considerations in post-training is around balancing your data and your rewards
so that you can get the right mix of generalization and specialization.

One of the biggest trade-offs in AI is around generalization versus specialization. Basically,
how do you make a model smarter without making it dumber? This really isn't just an engineering
challenge, it's also a pretty fundamental tension that every frontier AI lab faces
daily, and it also is a challenge when you're doing any kind of post-training.

And this is where data mixing really comes into play, and you have to consider how you actually
want the data to be mixed, what kind of percentage of each type of data you want in there to get the
exact right concoction for the right type of task that you want your model to perform.

Say you have a model trained 90% on coding questions and 10% on general text. In your error
analysis, you find that it's really great at coding, but it starts to fail at simple,
everyday queries like, what is the color of the sky? And so, this is a data mixing problem. So,
it's really, really good at where you've honed in on that data, but you don't have enough general
text in there. So, maybe it's time to rebalance that training dataset to 60% general QA,
30% coding, 10% safety. Ultimately, this is really empirical, right? So, you want to actually test
a few of these. And typically, what I would do is run a few different experiments at different
quantities at a small scale and get a sense of what mixing, what kind of percentages we actually
need for the ultimate use of the model. Ultimately, this is going to be determined by how
your users are using the model and how much they're using it for each of these tasks.

Sometimes, it's going to be a trade-off between different tasks and different capabilities.
For example, I've trained a translation model before, and we could make the model really,
really good at these low-resource languages if we had it in the data mix, but then it actually
made it worse the main languages. And so, that made it really tough to ship in production when
most of our users were using it on the main languages. And so, that was an opportunity
to probably use either different LoRA adapters, for example, for the different tasks or different
models entirely. Rewards and your reward function kind of operate in a similar way. So, maybe you

have a problem where your reward model maximizes only helpfulness and the model kind of overshares.
Maybe it sometimes has unsafe or speculative answers, like just giving medical advice.

Here is where you might want to balance your reward function a bit and redefine
your reward to put less weight on that helpfulness piece and more on safety.

And this weighting will just align that behavior to your actual deployment values as well. And
again, this is very empirical across RL training and test environments. So, you need to be really,
really disciplined about creating the right type of test environment so you can trust what that
output actually is and you can trust which experiment you're going to actually run with,
ultimately. So, some other problems. So, you might have a problem where your reward for
conciseness has gone too far. The model is just too concise all the time. Why is sleep important?

It's healthy, is all it says. And then, as a result, you have this trade-off between this
conciseness that is really great at answering very clear, easy questions, and it's not too
verbose and not annoying like that. But then, it's not complete, right? So, there's a trade-off here
that you might have to make. But you can also address this by having more targeted additions
to fix the scenario. So, maybe there are areas where you want it to be concise and certain
domains where you want it to be concise, but there are other areas, other domains where you don't
need that. Then, you can add data or rewards to carve out those exceptions where one-liners are
good or bad. For example, if they're good for trivia or like factual question answering, like,
what's the capital of France? Paris. Concise is really ideal. But then, when it's explanatory
or causal questions, like, why is sleep important? Then, you show examples with a lot of depth.

You can also have a case where the model actually avoids both overly short or verbose answers by
just asking for clarification. So, you can teach the model to ask for that clarification in order
to give a better response to the user. So, maybe you've trained a math specialist and you're on
your day 100 of training. Maybe it's not that extreme, but it has learned a lot of complex
calculus. It is able to prove these math theorems at a really advanced level, but it started to lose
some of those basic capabilities. And this is a trade-off you're going to have to make at some
point where your model starts to gain these specialist capabilities. At the same time,
one thing I want to emphasize is you might actually not need some of these law skills anymore.

So, one thing to really think about for your evals is, are you going to look at the evals
online on the web for the general purpose capabilities, or are you going to look at
evals specific to your business and what you actually need your model to accomplish?

As said before, balancing is really an empirical process. And one of the most important things you
can do here is scale it out based on your different experiments. Another important thing to keep in
mind is that the empiricism for this process never ends because your data distributions,
how your users will use your model, will always, always shift in production. And that's just a
reality. So, you have to be really, really good at setting up those experiments and being prepared
to actually adjust to your distributions as they do shift.

Congratulations! Now you've learned about the algorithms, the evaluation,
and the data. Now it's time to tie it all together and put it in production.
