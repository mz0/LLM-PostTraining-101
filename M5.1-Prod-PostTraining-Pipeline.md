Welcome. Now it's time to take a look at the important considerations for production and putting your model in production.

Let's look at a full production post-training pipeline that a frontier lab uses.

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Jan 2025]

So, as you've seen before, frontier labs have pretty complex pipelines for post-training.

You're going to deep dive into one of them right here from DeepSeek.

This is DeepSeek's R1 and R1 Zero, and these were the first examples of open-source reasoning models.

And this was published early 2025. And here's what the pipeline looks like.

[slides 3-4 from https://arcprize.org/blog/r1-zero-r1-results-analysis]

There are two final models here. So, DeepSeek R1 Zero and DeepSeek R1.

There are a bunch of intermediate models that they use to get there.

There are a bunch of different datasets that you've already explored in a previous module.

And, of course, there are several post-training stages that utilize those datasets and create some of those intermediate models up to that final model.

So, let's start with the R1 Zero pipeline.

This model is trained only with reinforcement learning, only with RL, with verifiers only, so no reward model even.

It's simple, just these three steps.

But it's helpful to understand these steps and then be able to compare it to that larger DeepSeek R1 pipeline.

So, first, starting with the base model, this is DeepSeek v3 base.

This is a base model that they had previously pre-trained.

Everything is kind of starting off from this guy at the top.

[slide 10 from “DeepSeek-V3 Technical Report”, DeepSeek-AI, 2024]

Deepseek-v3 base was pre-trained on 14.8 trillion tokens, so on a huge amount of data.

It took a lot of time. I mean, they use the word only several times, so only 180,000 GPU hours.

And at the time, it was a groundbreaking result because it's only $5.3 million, which was technically not a lot for pre-training,
considering how compute-intensive pre-training actually is.

They used H800 GPUs here, and they were looking at like a $2 an hour kind of cost to get that 5.3 million.

And what's really interesting and exciting is that DeepSeek v3 performance actually compared very favorably
against other base models around that same time.

So, you can see here's the benchmarks across those other models.

Great. So, what did they use that base model for?

[slide 11 from https://arcprize.org/blog/r1-zero-r1-results-analysis]

They took that base model, and then they did this RL stage, and that was it.

It's reasoning-oriented RL using GRPO, which you've learned about before.

And these are just rule-based rewards for accuracy and formatting.

[slide 12 from DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Jan 2025]

The training data was specifically just math and coding related.

GRPO only had two verifiers.

One was for accuracy for that math or code, and then the other one is for formatting: are the `think` tags in the correct place.

On the AIME math benchmark, they were able to get from 15.6% accuracy to 86.7%,
which is a very big jump for an open-source model and was competitive with OpenAI's O1 model at the time.

That made it a very interesting model and a huge splash in the media and in the ecosystem at the time.

And what's really interesting here is to see this graph.

So, this graph is honing in on the fact that the model only trained with RL and these verifiers.

So, there were no demonstrations for input or target output for fine-tuning.

There was no reward model giving any feedback back.

The model really could think for as long as it needed to to give an appropriate output, appropriate answer.

And they found that, naturally, **the model started to think more and more**, **reason more and more**,
**spend more tokens** in its think tags to get to the right result over time and training.

And that is what this graph is showing.

It's showing the length of that response increase over time.

And so, it's thinking more and more.

[slide 13 "Behaviors such as reflection… the exploration of alternative approaches to problem-solving arise spontaneously."]

Just as you saw in that graph, what they found was that behavior such as reflection,
so the model reflecting on its own kind of in an agentic way,
or the exploration of alternative approaches to problem-solving actually arise really spontaneously using this type of RL.

And this is one of the experiments that really *isolates reinforcement learning* from just *fine-tuning*.

It kind of shows what RL is capable of in this case.


[slide 14 add "DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. Time to upgrade R1-Zero to R1"]

However, on its own, this model did struggle with a lot of different challenges as well.

It had poor readability.
It would mix languages up a lot to get to the right response.
And maybe that was the most efficient way for it to get there.
But it actually resulted in a much less usable model, at least for people to understand what it was thinking.
So, it was harder to audit.

It also was just only contained in these math encoding domains where we have those verifiers.
So, it was limited, but it was a very good kind of experiment to see what the capabilities of this technology are.
So, it's time to upgrade from the R1 Zero model to their R1 model.


[slide 15 - R1 pipeline: General purpose, not just reasoning]

That pipeline is much more complex.
The goal is to be, of course, more general purpose, not just reasoning.
So, you need both reasoning and non-reasoning data.
And you need that to be mixed in in the right ways.


[slide 16 - R1: Generating reasoning data] So, starting with the reasoning data pipeline.
 
[slide 17 - Generating reasoning data]

First, you start with the *cold start* long *chain of thought* data that you've seen before.
And just *fine-tune* that a little bit, a couple *epochs*.
And these are just very detailed reasoning chains in this dataset just to get the model warmed up to reasoning.

[slide 18 - intermediate fine-tuned model] - ready to be trained with RL.

So, it doesn't go fully off the rails.
The fine-tuned model is a little bit more on track to reason.

[slide 19] here is adding that chain of thought language and consistency reward.

So, making sure that the model is actually outputting just one language.
They're trying this now so that it is rewarded from a reinforcement learning perspective to actually output only one language at a time.
And that makes it more auditable.

[slide 20] - And then you get another model.

Finally, using that other model, you can then create additional data here.
And these are more reasoning prompts generated by this model.
It's just synthetic data here.

Then using rejection sampling -- sample multiple out of this model and only pick the best ones, --
they're able to get a distilled, really high-quality reasoning dataset.

Using a lot of different rules to filter and using other models to filter here, the base model as a judge here,
they're able to get this reasoning dataset of about 600,000 examples. That was the reasoning pipeline.

[slide 21 - R1: Generating non-reasoning data]

Now on the non-reasoning data side, they're able to generate these chain of thought prompts using the DeepSeek-V3 base model.

They are able to get the model to, have these long chains of thought,
but not quite towards reasoning tasks like math or code.

And they're able to get these target outputs and then use this input and target output in that non-reasoning dataset.
They also combine it with direct answer data.
So, you see that on the left here with DeepSeek v3 fine-tuning data.
And that's also added into that non-reasoning dataset.
Okay. So, that was your non-reasoning data pipeline.

[slide 24] Now you *combine the non-reasoning dataset and the reasoning dataset* together.

[slide 25] And that creates a combined dataset of about 
800,000 examples that first feeds into fine-tuning for 2 epochs there,
creates this intermediate model that then feeds into the reinforcement learning pipeline,

where there are reasoning and preference rewards.

There is a reward model here and very diverse training prompts.

[slide 26] And that gets you your final DeepSeek-R1 model.

And that is the entire pipeline.

Now that you understand all the different components that go into a production post-training pipeline,
let's take a look at agents and how they interact with the real world.
