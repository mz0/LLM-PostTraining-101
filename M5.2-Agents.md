0:01
Agents are a really popular way to deploy LLMs in production.
0:05
A live agent can actually help inform what type of behaviors you need to do in post-training
0:10
to get the right result.
0:12
LLMs that are live in production today go beyond your average assistant.
0:16
So, you know, your average assistant learns to chat with post-training,
0:19
it learns to interact with a person.
0:22
But an agent is something that is often in production
0:25
or something that people want to put in production,
0:27
and post-training enables a lot of the different capabilities in agents as well,
0:31
like using tools and planning and coordinating,
0:33
but it's a different type of post-training,
0:35
and you've seen a little bit of that already.
0:37
And this is because in agents, their UX, their user experience,
0:41
is different than that of a chatbot.
0:43
And specifically here, chatbots, you know,
0:46
they're good at responding to different queries, having a good conversation,
0:49
they can handle that chat history that you saw before that post-training enables.
0:52
But with different post-training,
0:54
you can get an agent that can use tools and APIs,
0:56
which you've seen a little bit before too, to reason, you've seen that,
1:00
and be able to loop and have fewer hallucinations in that loop
1:04
and learn to basically do that reflection loop more effectively.
1:07
And finally, it's able to coordinate.
1:09
So, you'll go through each of these,
1:11
but essentially agents interact with many components of the real world,
1:14
and it can be messy, right?
1:16
The information can be messy, the tools it's using could change over time.
1:19
So, let's take a look at how all of these components work,
1:22
but then also how this all culminates in a live agent in production.
1:25
So, first for tool use, you've seen this before.
1:28
So, essentially, what is today's weather?
1:30
The target output could actually use a tool like the weather API today
1:34
and then show something and display something here.
1:37
You can use these fine-tuning examples to get your model to become more agentic.
1:42
And then RL for tool use, similarly,
1:45
you can teach it to use a calculator here instead of calculating it on its own
1:49
and get a reward for either, you know, getting the right answer
1:52
and or also using the tool itself.
1:55
For RL, you've seen this also using the search API, using code-based files.
1:59
So, using the tool of search on the internet to get updated information.
2:03
So, this is all for tool use.
2:05
This is all post-training for tool use.
2:07
Next is planning, and I think that largely falls into reasoning
2:10
and how a model should think step-by-step to get to a better answer.
2:14
And so, for fine-tuning for reasoning, you've seen the chain of thought before.
2:17
For RL, you've seen it as well, getting a reward for the final right answer
2:21
and allowing it to plan.
2:23
For coordination, I think this takes it to the next level,
2:27
but you haven't seen as much before.
2:29
But this shows how post-training could occur on a multi-agent transcript.
2:34
So, essentially, you could have a model be able to operate better with other agents
2:39
that are using other tools.
2:40
For example, here, agent A might be showing,
2:43
okay, it's able to break down the math problem.
2:46
Agent B is in charge of the calculator tool and using it.
2:49
And then your target output is thinking about,
2:51
how do I aggregate this information from agent A and agent B together
2:55
to get to the right answer?
2:56
That's the final model you're fine-tuning here.
2:58
And so, it's using multiple different possible models or possible agents
3:02
to get to this final target here in fine-tuning.
3:05
And actually, these are all possible different tasks,
3:07
so you can actually teach the model to be agent A, agent B, and agent C as well.
3:12
In RL for coordination, what it could look like is
3:14
maybe the user is saying, find refund status for order number 123.
3:18
And the model outputs, it is looking to agent A,
3:21
so it's using agent A in some way, which, again, could be its own sub-agent.
3:26
But it answers, oh, sorry, I didn't get the order info.
3:29
And what you want to teach the model is to be able to hand off information
3:33
correctly across its sub-agents or to another agent and to get to the right response.
3:38
For example, here's another bad case.
3:40
So, if the agent A said there's a refund pending,
3:43
even though there was no refund found and then you refunded it,
3:46
that's not great either.
3:47
You don't want to refund the order if that doesn't exist.
3:50
Versus here is the correct case where agent A found that this order belongs
3:54
to a certain customer ID, and then that customer is then refunded.
3:58
Great. So, this kind of brings us to live agents.
4:01
So, as we deploy these in production,
4:03
what are some of the different considerations that need to happen
4:06
for this agent to work effectively?
4:08
So, one is just the state is constantly updating.
4:11
So, that is something that is more difficult in production, right?
4:14
Your API changes, your tools change themselves, state is just constantly changing.
4:18
So, being able to manage and handle that effectively is really important.
4:22
New context.
4:23
So, there's new information coming in all the time,
4:26
and it's not just frozen in time when the model was trained on that data.
4:30
So, instead, this is like using a search API where you get news information,
4:34
new information, or you can use retrieval augmented generation or RAG,
4:38
which is essentially a data search to add relevant info into the input
4:41
that is maybe more up-to-date.
4:43
Finally, there's messy data and data that can be wrong
4:46
that gets added to the model in some way in the context,
4:49
and the model needs to be able to handle that.
4:51
And these are all different things that this agent needs to be post-trained
4:54
to be able to handle.
4:55
So, based on the behavior that you need the agent to operate on in production,
4:59
you need to design your agent accordingly to be able to be robust to these things.
5:03
So, here's an example of, you know, continually updating tools.
5:06
So, you basically want the model to learn to use tools for updated state
5:10
and not to rely on its own internal state,
5:12
because its own internal frozen state is not going to be continually updated.
5:17
It's hard to have a continual post-training run all the time,
5:19
though I'm really excited about that in the future.
5:22
One very simple one is actually using a date-time tool,
5:25
because when the model is trained, it probably thinks it is in the past
5:29
as time moves forward.
5:31
And so, using a date-time tool to understand what is today's date
5:34
and leaning on that, making sure the model defaults to using that
5:37
instead of, you know, remembering what today's date is, is really important.
5:41
Same with using a search API and being used to actually using the search API
5:45
to get relevant information as opposed to relying on its own knowledge.
5:49
So, this is a behavior change that you want to teach using post-training.
5:53
Another behavior thing is to think about information
5:57
that the model has never seen before
5:59
and making sure the model is really comfortable with using that information.
6:02
So, RAG, Retrieve Augmented Generation,
6:04
basically can retrieve newly seen information from any kind of data source,
6:08
similar to using a search API.
6:10
And you can attach, like, a new earnings report here,
6:13
and the model should be able to handle that.
6:15
You should also be able to handle, like, if something is wrong, right?
6:19
So, if that earnings report was actually not an earnings report
6:21
or it's not the latest one, the model can actually then go check that, right?
6:26
So, the model here is actually checking what is today's date, right?
6:29
Actually, this is not the latest earnings report.
6:33
So, it was able to use that date time tool probably
6:35
and then check that this, in fact, was the new earnings report
6:39
and then handle that incorrectly provided information from the user,
6:42
which is probably pretty common in production.
6:45
So, here's just a little case study of how you might be able to
6:48
handle a live agent coordination situation,
6:52
and that informs what you do in your fine-tuning and RL workflows.
6:56
So, maybe a user is saying, you know, my order is late.
6:59
Tracking says it's lost, help.
7:01
And your model says, okay, I need to find the user's order and check the tracking.
7:05
And it kind of writes out this code as a result, but it gets this error.
7:10
Okay, so it still doesn't have the status.
7:12
It tries again.
7:13
Still error.
7:14
Okay, it's getting a problem.
7:16
Trying again.
7:17
Still error.
7:18
And then, finally, it answers the user, sorry, I just don't have that information.
7:23
Please check our FAQ page.
7:24
You're probably familiar with seeing a response like this.
7:26
And, of course, the user is very upset.
7:28
How do you get this information?
7:30
One is, a lot of the times, you're going to have to deploy,
7:33
even in a limited setting, an agent to collect this type of information
7:37
that the user is actually asking you for,
7:39
and then you need to correct it using post-training techniques.
7:42
For example, in fine-tuning, what this could look like is you have this error, right?
7:46
You could see that there are issues with maybe the function name
7:50
or even the parameter kind of being passed in.
7:53
And so you want to give the right target output and teach the model,
7:57
okay, this is actually the right function declaration,
8:00
and this is actually what you're supposed to be passing in,
8:02
the customer ID and not the name.
8:03
And you create a lot of examples of this
8:05
so the model can actually effectively use your company's tools.
8:08
In RL, what this could look like is this same error.
8:11
It doesn't execute, and it's wrong.
8:14
Maybe if it executes, you give it slightly higher reward.
8:18
It's still wrong, though.
8:19
And then if it's correct and it executes,
8:22
then you give it that really positive reward.
8:24
So that's what this could look like.
8:26
So in planning, what this could look like is, you know,
8:29
there are a lot of failures and it just quit,
8:31
and it said just look at the FAQ page,
8:33
and that maybe gets a negative reward
8:35
versus you might want the model to actually be escalating.
8:38
That might be a more effective path
8:40
for a human-in-the-loop kind of process in production.
8:44
And so for many failures,
8:46
the model actually is able to escalate it,
8:48
and then it gets a positive reward.
8:50
So this is just a way of thinking about,
8:53
okay, how do I actually need my agent to operate in production?
8:57
And then what tools do I have in my tool chest
8:59
and post-training with fine-tuning and reinforcement learning now
9:02
to actually adapt my model so that it can adjust for these things?
9:08
Now that you know how live agents interact with post-training,
9:11
let's switch gears a bit and take a look at the different stages
9:15
of promoting a model from development to staging to production
9:19
and specifically honing in on the RL piece.
