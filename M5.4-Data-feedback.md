# Data-feedback flywheel

5:43

One of the great things about AI is that the data you get from a production model with
your users actively using it can actually be harvested for the next generation of that
model in post-training.

Let's take a look at how that works.

So you've seen the error analysis flow before, where you review the different failures, you
cluster them, you have hypotheses for fixes, you implement those proposed fixes in experiments,
and then you run a bunch of experiments to then do that flow over and over again to improve
your model.

Now in production, what does that look like?

So you can actually review those failures and cluster them as well, and you're reviewing

them probably in a production setting, so the model's already out there in the world.

And then you have additional questions, right?

So how urgent is it?

How urgent is the issue?

Is it something that you feel panicked about because the user experience would be really
bad?

You would lose a lot of money, for example, if you start refunding a bunch of customers
when you shouldn't, right?

What other models exist?

What other models do you have deployed today?

Maybe you're A-B testing different models.

Can you pull one of the models back?

And what's the budget?

And so what kind of money, what kind of resources and time and people do you have to actually

fix this error?

Like, basically, how important is it?

And then from those things, you want to either re-deploy a patch or you want to record that
feedback to then be used later.

So typically what this is going to look like is you'll have some user feedback.

You'll also have a lot of logs from usage.

And then you'll do a lot of clustering there, just like before.

You'll clean it up.

There's a lot of additional data work here to transform it.

You might have some of those synthetic transform pipelines here.

And then you can use that data for downstream post-training experiments.

What feedback might look like?

You'll see this in your lab.

Essentially, you might be able to plot user satisfaction, also get a distribution of thumbs
up, thumbs down, or thumbs middle.

Clustering feedback.

You've seen this before with k-means clustering.

You can apply this as well.

And maybe you get a sense of the error distribution or error type distribution in this case of
JSON format errors, which you'll see in your lab versus outdated knowledge.

You'll spend a lot of time in your logs to get really rich examples.

This is a great way to utilize your production model to make it better.

So here's where you can actually identify both really good failures, but also really
good success cases as well.

You convert this into good fine-tuning pairs, for example, and you can use synthetic data
pipelines to expand that coverage.

So you might find some really good, like, ooh, this is really good because of one reason.

And then you can expand it further if you don't have that coverage for fine-tuning.

So as an example, you might have this input, and you might have different model outputs,
and you can actually pair good versus bad from the logs.

And you can basically find these examples by mining the logs for this preference data,
as an example.

Here's another one of correct, concise versus verbose.

Data hygiene is really important.

So you're going to have a lot of logs, and there's going to be a lot of noise.

So you want to be filtering pretty aggressively here.

So what that might look like is just removing a lot of the factually incorrect logs or irrelevant
examples and really honing on the examples that are correct.

This is where your synthetic pipelines for filtering could come into play very effectively.

Deduplication is a big piece of this.

So you don't want to have a ton of identical examples taking over your datasets for fine-tuning
or even RL, like the different inputs that you want to turn your model on.

So making sure you're de-duping effectively is important as well, and you've already learned
some of the tactics around that.

Checking for biases.

That's going to be something that's going to be important in production in particular,
and you'll hear from user feedback whether that is actually showing up, and checking
for those different sensitive aspects and dimensions that you care about.

Finally, one really important thing in production is that you're going to be getting probably
sensitive information in some way, and making sure you are scrubbing that personal information
is important for your model to actually be more trustworthy later on as well.

You don't want your model spitting out people's social security numbers.

So I think model providers are actually quite aligned with scrubbing this personal information
to use as trading data later.

So just to summarize, you're getting your user feedback, you're looking at your logs,
you're cleaning it up, and of course then you can use this for your data.

Now realistically, just stepping back a bit, there are different possible ways to intervene
for different fixes.

I actually would suggest if you need to do a really big hot fix and just change very
quickly to think about prompt engineering, basically just changing your prompt, changing
your input, your basic template that your users are using to adapt to the model's behavior,
to do that first because that will be a really quick intervention of just a day.

Improving and updating your RAG index also would be a way to really quickly improve the
experience for the user for outdated information.

And for post-training with fine-tuning and reinforcement learning, it's going to take
about a week relative to actually get some results back because you're running many,
many different experiments to actually see what that improvement could look like, even
if you have all the infrastructure ready in place to do that.

So just to summarize, these are the types of issues that you might see and the types
of interventions that might be a good first pass.

So if you see a lot of unseen topics, you probably do need to dive into fine-tuning
and actually provide examples to the model to understand them.

If you have outdated knowledge, RAG is a really good first pass there.

If your users' preferences really shift, if they shift really heavily and maybe your
model is really sycophantic, for example, that is what OpenAI's GPT-4o was, then you
can use reinforcement learning to actually shift that preference over so the model is
more balanced.

If you have small consistency issues, small issues, I always recommend starting with prompt
engineering just to start to see if you can nudge the model sufficiently.

And then if you have finally a major skill issue, a capability gap, that is where you
apply both fine-tuning and RL.

Now that you have your feedback-to-data flywheel going, let's take a look at the observability
you need to put in place for your production model.
