# Monitoring and observability

Let's take a look at how this impacts your post-train model.

Alright, so there are a bunch of things you want to monitor in production.

There's performance, there's cost, and there's reliability.

And under reliability, you also probably want to be monitoring for a lot of the behavioral changes in the model,
because that will impact post-training the model further.

And around cost, that will also affect post-training, for example, your LoRA size or your base model size of what you pick.

So in your code, this is what it could look like.

Just monitoring all these different aspects of how the model is doing and how your users are interacting with the model.

Creating graphs is a good way to understand at a glance what's going on.

Here's a possible latency distribution graph that you'll see in your lab.

And also token usage distribution, just to understand, oh, how are people really using this model?

And how can I then use this information to create the right types of examples?

For example, here you might want examples that are really low and very high token distribution in your dataset to represent how users are using it.

So here's a production cycle again.

I've generalized it a bit from just the RL promotion rules to just post-training in general.

So in your experimentation, then you're evaluating on your test sets and test environments, and then you go to staging and production.

And the monitoring of the production metrics is happening there in production.

Of course, you can also monitor some of this in staging.

Okay, so for things to be really reliable, one of the most important things is around version control.

Because when you're monitoring, you see something wrong, you want to roll back to a previous version, you want that to be possible.

And in order for that to be possible, there's a lot of different things you have to make sure are stable and frozen,
and that you can actually do a reliable rollback, just like in regular software.

And that is happening here in production as well.

But versioning is really just a foundation.

You also need a monitor for several other different things.

So data drift, input data will always change.

People will always use the model differently.

Maybe they use different terms, or they have different concepts that are relevant to what your model should know about.

Maybe the laws change in some way, and the model needs to adjust to that.

The world is no longer the same.

An example is that after the pandemic, queries for remote work surged, and you need to be able to handle that.

The model needs to be able to handle that in some way.

So reward collapse is where your RL model is starting to gain metrics instead of optimizing for actual user satisfaction.

And that will also happen as well, and it's something that you need to monitor very closely for.

One very common example in general AI is high click-through rate from clickbait will lead to low satisfaction for users,
but it will get that high click-through rate that the model is trying to optimize for.

And finally, data quality, infrastructure problems, and model forgetting will over time cause degradation
and might be hard to actually see before you have it in production at a large scale.

And so there's been recent research around different infra problems that might actually cause the model to do worse than it actually is capable of.

And so getting all the infrastructure in the right place is important as well.

When you have multiple models, it's also helpful to be able to test them against each other.

So A-B testing allows you to do that very live by segmenting live users to understand which model is actually doing better for which different types of users.

And that A-B testing is probably happening here in production for the different passing models.

Next, a side-by-side comparison.

You might have seen this in ChatGPT or a different AI model that you've been using,
but essentially you can see comparisons of one response or another, and you can actually have human evaluators pick one,
and those can be just your users actually at a large scale.

And this can help you with quality checks.

You might have also seen in some of your email apps that they give suggestions for a possible response,
and they give multiple suggestions.

That is also a very natural way to have the side-by-side comparison with users.

And that's probably happening in like a staging phase.

Obviously, it could also happen in production just to harvest that data back to the experimentation loop.

Finally, it can be really helpful to have internal playgrounds for a limited set of users as you safely deploy these models out.

And these are just areas where you can have people try to break these models and identify edge cases and failures.

And that's typically done in the staging phase, so you can have internal playgrounds,
and then you can also have some beta testing external playgrounds on that live canary traffic.

Now that you understand the model production lifecycle, let's take a look at the infrastructure to make all that magic happen.
