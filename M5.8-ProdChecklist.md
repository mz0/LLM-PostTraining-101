**Production considerations**

# Production-ready checklist

Just to make sure everything is squarely in production,
let's run through your production checklist.

So how do you know you're ready?

Well, here is a checklist for production
that you should go through
before you're ready to ship your model out there.

So first is that clear reproducible model config.

So you want a reproducible config
to actually get the model
that you are releasing out into production.

You want it to be reproducible.

You want to be able to re-run it at any time
and be able to get the same numbers
so that you know for a fact that this is your model
and how you got that model.

So the goal is that you can run it
on all the held out test sets and suites
and get the same numbers.

And within some, of course, confidence interval tolerance.

So that's number one.

So you can trust the model that you're putting out there.

Boom.

Next up is promotion roles and rollback conditions.
And set up across your different stages
from development to staging to production.

So you saw that with the RL promotion roles
and the same thing with just generally
your fine-tuned model as well.

But afterwards, you're turning your North Star here
into an actual contract with what's go or no go.

So what are you actually going to use as your gate
between development and staging
and then staging to production?

And getting really clear
about what your promotion roles are
and codifying that, writing that down,
makes it so that you can automate a lot of these pieces
of what model then goes out
and then how you even roll back.

Next is monitoring and observability.

You looked into these a bit,
but for different slows or service level objectives,
being able to understand what the behavioral
as well as the system metrics are
for the different slices, for example, that matter for you.

And to understand just how well is this model doing
out in the wild?

So what is its behavior?

And then what kind of actions can you take
based on that behavior?

Next is your feedback to data pipeline
or that flywheel that we discussed before.

Here, the goal is to turn any failures
or even good things that you see
from how users are using your model
into better high-signal supervision
for the next round of fine-tuning and RL.

So you're looking at your logs, you're mining them,
and then you're clustering them,
you're understanding the patterns in them.

And then, of course, using that for downstream data,
which you can use synthetic pipelines
to generate coverage for,
and then you queue for training for another experiment
for both fine-tuning and RL.

And finally, your infrastructure.

So how you allocate your infrastructure
is really important, too, to make it possible
for you to run your different experiments
but also to make sure you serve your users appropriately.

So you want to meet targets on your GPU scale-out
elastically, and you don't want to regress
on those important behaviors as well.

And here is where you're looking at throughput,
latency, and, of course, cost.

Great, so now you have your production checklist.

It's time to go roll out your model.

Congratulations on getting to the end
of the course of post-training.

You've gone from just understanding the very basics
and intuitions of reinforcement learning and fine-tuning
to understanding the nitty-gritty math behind them
to using evaluation as the North Star
to guide your post-training,
to the data and the important data considerations
you need to make, to finally production
and getting your model out there into the world.
