{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b4193e-c083-4472-bef7-1db91d543fec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Graded Lab: Monitoring, Diagnosis and Interventions in Production\n",
    "\n",
    "Welcome to this assignment!\n",
    "\n",
    "Carefully read each Markdown (text) cell, which include instructions and hints. Start by reading the background behind your upcoming tasks.\n",
    "\n",
    "When you are done, submit your solution by saving it, then clicking on the submit button at the top right side of the page.\n",
    "\n",
    "## In order for your submission to be graded correctly, you **MUST**:\n",
    "* **Use the provided variable names**, otherwise the autograder will not be able to locate the variable for grading. \n",
    "\n",
    "* **Replace any instances of `None` with your own code.** \n",
    "\n",
    "* **Only modify the cells that start with the comment `# GRADED CELL`**.  \n",
    "\n",
    "* **Use the provided cells for your solution.** You can add new cells to experiment, but these will be omitted when grading. \n",
    "\n",
    "To submit your solution, save it, then click on the blue submit button at the top of the page.\n",
    "\n",
    "<div style=\"background-color: #FAD888; padding: 10px; border-radius: 3px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); width:95%\n",
    "\">\n",
    "<strong>Important notes</strong>:\n",
    "\n",
    "- Code blocks with None will not run properly. If you run them before completing the exercise, you will likely get an error. \n",
    "\n",
    "- The notebooks work best in Chrome browser. If you are having problems, please switch to Chrome.\n",
    "\n",
    "- Make sure you always save before submitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abed153-c81a-4a74-8ceb-b04993ea3c87",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will\n",
    "1. Explore a simulated production log dataset (`csbot_production_logs.csv`).  \n",
    "2. Define and visualize monitoring metrics (latency, token usage, error rates, satisfaction).  \n",
    "3. Diagnose and cluster failure cases.  \n",
    "4. Map problems to the right optimization interventions.\n",
    "\n",
    "# Objectives\n",
    "\n",
    "You will implement a complete production monitoring and intervention pipeline for LLM systems by analyzing real production logs, computing key performance metrics, diagnosing failure patterns, and mapping problems to appropriate optimization strategies. Through hands-on exercises, you'll learn to build robust monitoring systems that can detect issues early and guide targeted improvements.\n",
    "\n",
    "* **Compute Production Monitoring Metrics:** Calculate essential performance indicators including latency percentiles, token usage, error rates, and user satisfaction scores from production log data.\n",
    "* **Implement Alert Threshold Checking:** Build an alerting system that compares computed metrics against predefined thresholds and generates human-readable alert messages for production issues.\n",
    "* **Extract Failure Cases from Production Logs:** Define and identify failure cases based on user satisfaction scores, explicit negative feedback, and critical system errors to focus improvement efforts.\n",
    "* **Categorize Issues Using Heuristic Rules:** Implement rule-based classification to group failure cases into actionable categories like verbosity, politeness, knowledge gaps, and format errors for targeted interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173b2d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Data Exploration](#dataexploration)\n",
    "- [Monitoring Metrics](#monitor) - Exercise 1, 2\n",
    "- [Issue Diagnosis & Clustering](#issue) - Exercise 3, 4\n",
    "- [Intervention Strategies](#interventionstrategies)\n",
    "- [Summary](#summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0e3de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setup <a id=\"setup\"></a>\n",
    "\n",
    "As usual, start by importing the necessary packages. Here you also define a function to run tests."
   ]
  },
  {
   "cell_type": "code",
   "id": "10070375-8ea4-4da5-bcc7-7aa811157be7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def run_test(func, *args):\n",
    "    try:\n",
    "        func(*args)\n",
    "        print(f\"‚úÖ {func.__name__} passed\")\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå {func.__name__} failed:\", e)\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úÖ Setup complete. All required packages are available.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2c662e7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Data Exploration <a id=\"dataexploration\"></a>\n",
    "\n",
    "You will investigate the structure of production logs for an LLM-based system to detect missing or noisy values and think about their impact on downstream monitoring. You will also build intuition about **user feedback distribution** (satisfaction & thumbs) and **error type frequencies**.  "
   ]
  },
  {
   "cell_type": "code",
   "id": "03b9906c",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Load dataset\n",
    "logs = pd.read_csv(\"/app/data/csbot_production_logs.csv\")\n",
    "\n",
    "# Preview first 5 rows\n",
    "logs.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e64bcb75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Before you dive into analysis, understand **what information this dataset contains** and why it matters.\n",
    "\n",
    "The production logs include the following key columns:\n",
    "\n",
    "- **timestamp** ‚Äî when the request happened. Useful for time-based trends.\n",
    "- **prompt / llm_response** ‚Äî the user's query and the model's reply. This helps connect metrics to real examples.\n",
    "- **latency_ms** ‚Äî how long the model took to respond (in milliseconds).\n",
    "  - **average latency** measures typical performance.\n",
    "  - **p95 latency** (95th percentile) shows \"worst-case\" delays that real users may experience.\n",
    "- **tokens_generated** ‚Äî how many tokens the model produced.\n",
    "  - Tracking the **distribution of token usage** tells if responses are too long (which increases cost and slows down serving).\n",
    "- **error_type** ‚Äî system-detected issues (e.g., malformed JSON, outdated knowledge, style problems).\n",
    "  - From this you can compute **error rates** for each type, which show reliability problems.\n",
    "- **satisfaction / thumbs_up** ‚Äî direct user feedback.\n",
    "  - **Satisfaction scores** (1‚Äì5) show overall user experience.\n",
    "  - **Thumbs up/down** gives a quick binary indicator of good vs bad responses."
   ]
  },
  {
   "cell_type": "code",
   "id": "66041d55",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Inspect dataset\n",
    "logs.info()\n",
    "logs.describe(include=\"all\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5556cf0b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, **check data quality**.  \n",
    "- Are there any missing values in key fields like `prompt`, `llm_response`, or `satisfaction`?\n",
    "- If you find missing values, how should you handle them (drop rows, fill with defaults, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f6c26b9",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Missing values check\n",
    "logs.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "475e2565",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now look at **user feedback**.  \n",
    "- How are satisfaction scores distributed (1‚Äì5)?  \n",
    "- How often do users give a thumbs up, thumbs down, or no rating?  "
   ]
  },
  {
   "cell_type": "code",
   "id": "bb7ba799",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Satisfaction & Thumbs distribution\n",
    "print(logs['user_satisfaction'].value_counts().sort_index())\n",
    "print(logs['thumbs_up'].value_counts().sort_index())\n",
    "\n",
    "logs[['user_satisfaction','thumbs_up']].hist(figsize=(10,4))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c61ed0d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Check the **error_type column** to understand what kinds of problems occur.  \n",
    "Examples:  \n",
    "- `knowledge_outdated` (model gave old or wrong info)  \n",
    "- `json_format_error` (invalid output format)  \n",
    "- `too_verbose` (answer too long)  \n",
    "- `too_polite` (style issue)  "
   ]
  },
  {
   "cell_type": "code",
   "id": "0a9c93d3",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Error type frequency\n",
    "logs['error_type'].replace(\"\", \"none\").value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6de524d4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finally, look at a few **real examples** of prompts, responses, and feedback.  \n",
    "Reading raw examples helps connect metrics (like satisfaction score) with actual user experience.  \n",
    "This step builds intuition about **why** users like or dislike certain responses."
   ]
  },
  {
   "cell_type": "code",
   "id": "20b912ff",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Sample inspection\n",
    "sample_low_sat = logs[logs['user_satisfaction'] <= 2].head(5)[['user_prompt','llm_response','error_type','user_satisfaction','thumbs_up']]\n",
    "sample_low_sat"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc6aae86-06cf-427e-b56a-82d0a62db804",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Monitoring Metrics <a id=\"monitor\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36db32-9185-4e1d-b032-1f64d3c4edce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Load the production logs. If `logs` is already in memory, this cell will not overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "id": "163be157-284f-47f0-adef-52b39b7691b5",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Try Customer Service Bot dataset first\n",
    "try:\n",
    "    logs\n",
    "except NameError:\n",
    "    try:\n",
    "        logs = pd.read_csv(\"/app/data/csbot_production_logs.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Note: data file lost, please reset your environment\")\n",
    "        exit(1)\n",
    "\n",
    "display(logs.head())\n",
    "print(f\"Rows: {len(logs)} | Columns: {list(logs.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a299db3e-badf-4255-80a6-ef20948076c5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Convert the string `timestamp` column to a datetime column `ts`.  \n",
    "This is required for time-series visualizations later."
   ]
  },
  {
   "cell_type": "code",
   "id": "51253211-f35e-4088-b70d-aaff5b02b7c5",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import parse_timestamps\n",
    "\n",
    "logs = parse_timestamps(logs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ca54a22-4340-431c-9e4d-ddb60aa68ef2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Create a normalized error column `_error_norm` where empty strings are mapped to `\"none\"`.\n",
    "This makes error counting robust and consistent."
   ]
  },
  {
   "cell_type": "code",
   "id": "32426b87-eb01-47f1-a56f-5e2a2057e399",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import normalize_errors\n",
    "\n",
    "logs = normalize_errors(logs)\n",
    "print(logs[\"_error_norm\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3a866f5-b89a-46d1-83fb-fd666e694c5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 1: Compute Production Monitoring Metrics\n",
    "\n",
    "Compute and return a dictionary with:\n",
    "- `avg_latency_ms`, `p95_latency_ms`\n",
    "- `avg_tokens`\n",
    "- `error_rate_pct`\n",
    "- `avg_satisfaction`"
   ]
  },
  {
   "cell_type": "code",
   "id": "62a7b517-d67c-4720-bd89-6eb5cff706e1",
   "metadata": {
    "tags": [
     "graded"
    ],
    "deletable": false,
    "editable": true
   },
   "source": [
    "# GRADED CELL: exercise 1\n",
    "\n",
    "def compute_metrics(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Compute monitoring metrics:\n",
    "      - avg_latency_ms, p95_latency_ms, avg_tokens, error_rate_pct, avg_satisfaction\n",
    "    Returns a dict of floats rounded to 2 decimals.\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ###\n",
    "    # avg_latency is the average of response_time_ms, use .mean() to calculate the average\n",
    "    avg_latency = None\n",
    "\n",
    "    # p95_latency is the 95th percentile of response_time_ms, use np.percentile() to calculate the 95th percentile\n",
    "    p95_latency = None\n",
    "\n",
    "    # avg_tokens is the average of tokens_generated, use .mean() to calculate the average\n",
    "    avg_tokens = None\n",
    "\n",
    "    # error_rate is the percentage of rows where _error_norm is not \"none\"\n",
    "    error_rate = None\n",
    "\n",
    "    # avg_satisfaction is the average of user_satisfaction, use .mean() to calculate the average\n",
    "    avg_satisfaction = None\n",
    "    ## END CODE HERE ###\n",
    "    return {\n",
    "        \"avg_latency_ms\": float(round(avg_latency, 2)),\n",
    "        \"p95_latency_ms\": float(round(p95_latency, 2)),\n",
    "        \"avg_tokens\": float(round(avg_tokens, 2)),\n",
    "        \"error_rate_pct\": float(round(error_rate, 2)),\n",
    "        \"avg_satisfaction\": float(round(avg_satisfaction, 2)),\n",
    "    }\n",
    "\n",
    "metrics = compute_metrics(logs)\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7aba8616-cd9a-48c2-950f-7e527d127ed3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Unit Test: `compute_metrics` Implementation"
   ]
  },
  {
   "cell_type": "code",
   "id": "f1adce0b-205e-460a-bc22-cc7775638aa9",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.unit_tests import test_compute_metrics_returns_keys, test_compute_metrics_reasonable_ranges\n",
    "\n",
    "run_test(test_compute_metrics_returns_keys, logs, compute_metrics)\n",
    "run_test(test_compute_metrics_reasonable_ranges, logs, compute_metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01b50fcf-e6af-4768-b73a-bb109602c70a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Create functions to visualize different monitoring metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "id": "4f6854ee-d99a-4998-b2bf-9e3bdd38d10b",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import plot_latency_distribution\n",
    "plot_latency_distribution(logs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd0d6cc0-0d69-463f-8ef7-2cd72e4eec22",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You have already prepared plotting functions for the other metrics (`satisfaction`, `tokens`, and `error types`) inside **`utils.util.py`**.  \n",
    "\n",
    "üëâ Your job here is not to re-implement them, but to **use these utilities** to quickly generate the plots and focus on **interpreting the results**.  \n",
    " \n",
    "- **Token usage distribution** ‚Üí shows if responses are too short/too long (cost & efficiency).  \n",
    "- **Error type distribution** ‚Üí highlights the most common failure modes (reliability).  \n",
    "\n",
    "In the next cells, you'll call these ready-made functions and practice **explaining what the plots mean** in a production monitoring context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671ea65-260a-4693-9ada-c19c6cf16487",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the provided `plot_token_distribution(df)` function to visualize how many tokens the model usually generates per response.\n",
    "\n",
    "üëâ **Your task:**  \n",
    "- Look at the distribution. Do you see **two peaks** (bimodal distribution)?  \n",
    "- What might this mean? For example:  \n",
    "  - Some responses are short and efficient.  \n",
    "  - Others are very long (possibly too verbose), which may increase latency and cost.  \n",
    "\n",
    "‚úÖ **Key takeaway:**  \n",
    "Unusual clusters in token usage can indicate inconsistent model behavior that might need intervention (e.g., RL tuning for brevity).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "id": "85eeca33-482f-450d-9627-e9b637b1bef0",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import plot_token_distribution\n",
    "plot_token_distribution(logs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc4dca8c-6713-4e0e-98b0-06f4e573240f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the provided `plot_error_types(df)` function to check which error types are most common.\n",
    "\n",
    "- Which error type occurs most frequently?  \n",
    "- What could be the impact of each error type in production? For example:  \n",
    "  - `json_format_error`: breaks downstream systems expecting structured output.  \n",
    "  - `knowledge_outdated`: gives users wrong info, reduces trust.  \n",
    "\n",
    "Error distribution highlights the **reliability bottlenecks** of your system. The most frequent error type should be the first target for intervention."
   ]
  },
  {
   "cell_type": "code",
   "id": "a49a2a14-b849-461c-8946-170412275036",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import plot_error_types\n",
    "plot_error_types(logs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea420421-ad7d-4a74-86e8-0bd7c388ee70",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 2: Implement Alert Threshold Checking\n",
    "\n",
    "Compare metrics to thresholds and return a list of human-readable alert strings.\n",
    "Rules:\n",
    "- Trigger if metric > threshold for: `avg_latency_ms`, `p95_latency_ms`, `error_rate_pct`, `avg_tokens`\n",
    "- Trigger if `avg_satisfaction` < `avg_satisfaction_min` (already implemented)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d90bca3b-3e46-4b13-a1a6-807b2c7a6c5e",
   "metadata": {
    "tags": [
     "graded"
    ],
    "deletable": false,
    "editable": true
   },
   "source": [
    "# GRADED CELL: exercise 2\n",
    "\n",
    "ALERT_THRESHOLDS = {\n",
    "    \"avg_latency_ms\": 3000,     # Alert if average latency > 3000 ms\n",
    "    \"p95_latency_ms\": 5000,     # Alert if P95 latency > 5000 ms\n",
    "    \"error_rate_pct\": 25.0,      # Alert if error rate > 5%\n",
    "    \"avg_tokens\": 1200,         # Investigate verbosity if avg tokens > 1200\n",
    "    \"avg_satisfaction_min\": 3.0 # Alert if avg satisfaction < 3.0\n",
    "}\n",
    "\n",
    "def check_alerts(metrics: dict, thresholds: dict) -> list:\n",
    "    \"\"\"\n",
    "    Return list of strings describing which alerts are triggered.\n",
    "    \"\"\"\n",
    "    triggered = []\n",
    "    ### START CODE HERE ###\n",
    "    # Higher-is-worse metrics\n",
    "    for key in [\"avg_latency_ms\", \"p95_latency_ms\", \"error_rate_pct\", \"avg_tokens\"]:\n",
    "        # check if the key is in both metrics and thresholds and if metrics[key] is larger than thresholds[key]\n",
    "        if key in metrics and key in thresholds and None > None:\n",
    "            t = f\"{key}: {metrics[key]} > {thresholds[key]}\" \n",
    "            triggered.append(t) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lower-is-worse for satisfaction\n",
    "    sat_min = thresholds.get(\"avg_satisfaction_min\")\n",
    "    if sat_min is not None and \"avg_satisfaction\" in metrics and metrics[\"avg_satisfaction\"] < sat_min:\n",
    "        # check if metrics['avg_satisfaction'] is less than sat_min\n",
    "        t =  f\"avg_satisfaction: {metrics['avg_satisfaction']} < {sat_min}\"\n",
    "        triggered.append(t)\n",
    "    \n",
    "    return triggered"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d590f193-20cc-43c7-b896-467dd8b1a604",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "print(metrics)\n",
    "alerts = check_alerts(metrics, ALERT_THRESHOLDS)\n",
    "print(\"\\n=== Alert Check ===\")\n",
    "if not alerts:\n",
    "    print(\"No alerts triggered ‚úÖ\")\n",
    "else:\n",
    "    print(\"Alerts triggered ‚ùó\")\n",
    "    for a in alerts:\n",
    "        print(\" -\", a)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99c366fb-a6d0-4558-a684-da650fbc68ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Unit Test: `check_alerts` Implementation"
   ]
  },
  {
   "cell_type": "code",
   "id": "8165cae6-6481-496d-b4ef-90d1ae62e898",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.unit_tests import test_check_alerts_high_is_worse, test_check_alerts_low_is_worse_satisfaction\n",
    "\n",
    "run_test(test_check_alerts_high_is_worse, check_alerts)\n",
    "run_test(test_check_alerts_low_is_worse_satisfaction, check_alerts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4579a384-7049-4016-ad98-41e1da662998",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Issue Diagnosis & Clustering <a id=\"issue\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c7556-1c3f-4bf8-8232-9b006ee7ce20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Monitoring metrics (latency, token usage, satisfaction, errors) only tell us **when something is wrong**,  \n",
    "but not **what exactly is wrong**.\n",
    "\n",
    "To debug real-world LLMs, you need to define **failure cases** and then cluster them into meaningful categories.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454bbdfb-7c58-4854-9415-1556cad52ce5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 3: Extract Failure Cases from Production Logs\n",
    "\n",
    "**Failure case definition**:  \n",
    "- `user_satisfaction <= 2`, OR  \n",
    "- `thumbs_up == -1` (explicit thumbs down), OR  \n",
    "- `error_type` in {`knowledge_outdated`, `json_format_error`}\n",
    "\n",
    "Filter the dataset based on the failure definition and inspect the size and ratio of failure cases."
   ]
  },
  {
   "cell_type": "code",
   "id": "56cdb746-aa57-4c4d-b9e4-ac92fadcb7e5",
   "metadata": {
    "tags": [
     "graded"
    ],
    "deletable": false,
    "editable": true
   },
   "source": [
    "# GRADED CELL: exercise 3\n",
    "\n",
    "def extract_failure_cases(logs):\n",
    "    ## START CODE HERE ###\n",
    "    # Define three conditions for failure cases\n",
    "    # Condition 1: Low user satisfaction (1-2 indicates dissatisfaction)\n",
    "    low_satisfaction = None\n",
    "    \n",
    "    # Condition 2: Explicit negative user feedback (thumbs down), expressed as thumbs_up equals -1\n",
    "    negative_feedback = None\n",
    "    \n",
    "    # Condition 3: System-detected critical errors (error_type is either knowledge_outdated or json_format_error)\n",
    "    critical_errors = None\n",
    "    \n",
    "    ## END CODE HERE ###\n",
    "\n",
    "    # Combine all failure conditions (any condition being true marks it as a failure case)\n",
    "    failure_df = logs[\n",
    "        low_satisfaction | negative_feedback | critical_errors\n",
    "    ].copy()\n",
    "    return failure_df\n",
    "\n",
    "print(f\"Total logs: {len(logs)}\")\n",
    "failure_df = extract_failure_cases(logs)\n",
    "print(f\"Failure cases: {len(failure_df)} ({len(failure_df)/len(logs)*100:.2f}%)\")\n",
    "failure_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51b01c7b-638a-4fd1-883d-0f98d2a832f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 4: Categorize Issues Using Heuristic Rules\n",
    "\n",
    "Apply simple rules to label failure cases:  \n",
    "- `too_verbose`: tokens > threshold (e.g., 1500)  \n",
    "- `too_polite`: response contains [\"thank you\", \"thanks\", \"apolog\", \"sincerely\", \"appreciate\"] phrases  \n",
    "- `knowledge_outdated`: `error_type == knowledge_outdated`  \n",
    "- `json_malformed`: `error_type == json_format_error`  \n",
    "- `other`: none of the above  "
   ]
  },
  {
   "cell_type": "code",
   "id": "ee0556d8-96dd-4442-8531-5d8a2cb80e56",
   "metadata": {
    "tags": [
     "graded"
    ],
    "deletable": false,
    "editable": true
   },
   "source": [
    "# GRADED CELL: exercise 4\n",
    "\n",
    "def categorize_issue(row, verbose_token_threshold=1500):\n",
    "    etype = str(row.get(\"error_type\", \"\")).strip()\n",
    "    text = str(row.get(\"llm_response\", \"\")).lower()\n",
    "    tokens = int(row.get(\"tokens_generated\", 0))\n",
    "    ## START CODE HERE ###\n",
    "    # write if else statements to categorize the issue based on the row\n",
    "    if etype == \"json_format_error\": # @KEEP\n",
    "        return \"json_malformed\" \n",
    "    # If error type is knowledge_outdated, return \"knowledge_outdated\"\n",
    "    if None:\n",
    "        return None\n",
    "    # If number of tokens exceeds the verbose_token_threshold, return \"too_verbose\"\n",
    "    if None:\n",
    "        return None\n",
    "    # If the text includes any of the polite words, return \"too_polite\"\n",
    "    polite_words = [\"thank you\", \"thanks\", \"apolog\", \"sincerely\", \"appreciate\"] \n",
    "    if None:\n",
    "        return None\n",
    "    # If none of the above trigger, return \"other\"\n",
    "    return None\n",
    "    ## END CODE HERE ###\n",
    "\n",
    "failure_df[\"failure_cluster\"] = failure_df.apply(categorize_issue, axis=1)\n",
    "print(failure_df[\"failure_cluster\"].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a30a6e23-dd25-4805-a8b9-7ba0e118141f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Semantic Clustering (Optional, Advanced)\n",
    "\n",
    "Use **SentenceTransformer** to embed `user_prompt + llm_response (+ error_type)` and cluster failure cases into groups. This can reveal hidden patterns beyond heuristic rules."
   ]
  },
  {
   "cell_type": "code",
   "id": "b1fb897e-bc3d-4ff0-a1d4-d000bcb66309",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import semantic_cluster_failure_cases\n",
    "failure_df, info = semantic_cluster_failure_cases(\n",
    "    failure_df,\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    k_range=(2, 6),\n",
    "    use_error_hints=True,\n",
    "    label_col=\"semantic_cluster\",\n",
    ")\n",
    "\n",
    "print(\"Chosen k:\", info[\"k\"])\n",
    "print(\"Embeddings shape:\", info[\"emb_shape\"])\n",
    "print(\"Embedding backend:\", info[\"model\"])\n",
    "print(failure_df[\"semantic_cluster\"].value_counts().sort_index())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85eb873a-e225-4f8e-a6d9-b2b4153cbef0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Compare **heuristic categories** (`failure_cluster`) with **semantic clusters** (`semantic_cluster`).\n",
    "\n",
    "You will create a simple **crosstab** to see how many cases from each heuristic class fall into each semantic cluster.\n",
    "\n",
    "üëâ Look for patterns like:\n",
    "- Does one semantic cluster mostly correspond to `too_verbose`?\n",
    "- Are `json_malformed` cases concentrated in a single cluster?\n",
    "- Do `too_polite` cases mix with others?"
   ]
  },
  {
   "cell_type": "code",
   "id": "724eddcc-01aa-4f30-82ae-824ce6e39cfc",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Simple Crosstab\n",
    "if \"semantic_cluster\" not in failure_df.columns:\n",
    "    print(\"No `semantic_cluster` found. Run Step 3 (Semantic Clustering) first.\")\n",
    "else:\n",
    "    # Crosstab: rows = heuristic classes, columns = semantic cluster IDs\n",
    "    ct = pd.crosstab(failure_df[\"failure_cluster\"], failure_df[\"semantic_cluster\"])\n",
    "    print(\"=== Crosstab: failure_cluster √ó semantic_cluster ===\")\n",
    "    display(ct)\n",
    "\n",
    "    # Quick dominant mapping (which semantic cluster each heuristic class falls into most)\n",
    "    dominant = ct.idxmax(axis=1).to_dict()\n",
    "    print(\"\\nHeuristic ‚Üí dominant semantic cluster mapping:\")\n",
    "    for heur, sem in dominant.items():\n",
    "        print(f\"  {heur} ‚Üí cluster {sem}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb039527-62f8-4823-88e4-03ce340fa89f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Intervention Strategies <a id=\"interventionstrategies\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa38d7-8a4f-492e-bc7b-d3ce48e8bbae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "From your error diagnosis, you identified four major failure types.  \n",
    "Each of them connects to a **specific optimization technique** in post-training or production workflows:\n",
    "\n",
    "| Failure Type        | Problem Nature            | Optimization Technique                         | Why this works |\n",
    "|---------------------|---------------------------|------------------------------------------------|----------------|\n",
    "| **knowledge_outdated** | Knowledge freshness issue | **RAG (Retrieval-Augmented Generation)**       | Model itself cannot stay up-to-date. RAG lets us plug in fresh knowledge (FAQ, policies, docs) without retraining. |\n",
    "| **json_malformed**  | Reliability / format error | **Guardrails / Schema enforcement**            | Ensure outputs respect strict formats (e.g., JSON schema, function calling). Prevents malformed responses in production. |\n",
    "| **too_verbose**     | Preference misalignment    | **RL preference tuning / Prompt brevity control** | Users want concise answers. RLHF/DPO can optimize for brevity, or prompts can set max tokens / penalties. |\n",
    "| **too_polite**      | Style / tone preference    | **Prompt/template adjustments / RL tone tuning** | Over-apologizing or being overly polite reduces usefulness. Prompt templates or RL fine-tuning align tone with user needs. |\n",
    "\n",
    "Different problems require different tools.  \n",
    "You cannot fix knowledge freshness with RL, and you cannot fix malformed JSON with RAG.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e1246-bbf4-4006-8449-cbcd53cf84f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Below are some failure cases. Use the dropdowns to select the **best optimization technique** for each case."
   ]
  },
  {
   "cell_type": "code",
   "id": "fae7f259-6f44-44eb-9aee-d99645baef0f",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import create_intervention_selector\n",
    "\n",
    "# Create interactive intervention selector\n",
    "create_intervention_selector()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64271dbe-d0a5-47ef-b982-59c3de77e4af",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now imagine you are the **production owner**.  \n",
    "Given limited engineering resources, which optimization would you prioritize **first** for deployment?\n",
    "\n",
    "Use the dropdown below to make your choice."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad6e7bd4-08d9-4810-aea7-604cf2d1189f",
   "metadata": {
    "tags": [],
    "deletable": false,
    "editable": false
   },
   "source": [
    "from utils.util import create_priority_decision_selector\n",
    "\n",
    "# Create priority decision selector\n",
    "create_priority_decision_selector()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56b29a1b-4dcf-4a5b-ad86-9d824d584f51",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Summary <a id=\"summary\"></a>\n",
    "\n",
    "In this lab, you experienced the **end-to-end cycle of production model optimization**:\n",
    "\n",
    "This cycle doesn‚Äôt stop ‚Äî real-world LLM systems require continuous **monitoring, diagnosis, interventions, and iteration** to stay reliable and aligned with user needs.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/a32ebfb0-2c64-42d7-9028-b97b53d81eb7.png\" alt=\"Production Loop\" width=\"400\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "grader_version": "2",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}